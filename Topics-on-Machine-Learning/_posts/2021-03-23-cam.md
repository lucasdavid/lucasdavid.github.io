---
layout: post
title: Class Activation Mapping
excerpt: Explaining AI with Grad-CAM.
first_p: |-
  Gradient-based methods are a great way to understand a networks' output,
  but cannot be used to discriminate classes, as they focus on low-level features
  of the input space. An alternative to this are CAM-based visualization methods.
   
date: 2021-03-23 08:34:00
lead_image: /assets/images/posts/ml/explaining/cam/cover.png
tags:
  - ML
  - neural networks
  - Explaining
  - Grad-CAM
---

<span class="display-6">In</span> my last [post]({% post_url 2021-01-15-explaining-ai %}),
I talked about AI explaining in the computer vision area, and how to use the gradient
information to explain predictions for classification networks.
In short, if we consider our answer to be $a(x) = \max f(x)$, for whatever differentiable
network $f$ and input image $x$, we should be able to compute $\nabla a$. I.e., the gradient
with respect to the input $x$, composing a map of linear contributions
for each `RGB` value of each pixel in the image. Furthermore, we can obtain smooth gradients
by repeating this process $N$ times, adding a little noise each time.
The images below describe the application of VanillaGrad and SmoothGrad over multiple images.

<div id="carouselExampleControls" class="carousel slide" data-bs-ride="carousel">
  <div class="carousel-inner">
    <div class="carousel-item active"><img src="/assets/images/posts/ml/explaining/cam/grad/0.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/1.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/2.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/3.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/4.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/5.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/6.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/8.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/10.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/11.png" class="d-block w-100"></div>
    <div class="carousel-item"><img src="/assets/images/posts/ml/explaining/cam/grad/12.png" class="d-block w-100"></div>
  </div>
  <button class="carousel-control-prev" type="button" data-bs-target="#carouselExampleControls"  data-bs-slide="prev">
    <span class="carousel-control-prev-icon" aria-hidden="true"></span>
    <span class="visually-hidden">Previous</span>
  </button>
  <button class="carousel-control-next" type="button" data-bs-target="#carouselExampleControls"  data-bs-slide="next">
    <span class="carousel-control-next-icon" aria-hidden="true"></span>
    <span class="visually-hidden">Next</span>
  </button>
</div>

This works pretty well for images containing a single entity of the classifying set (e.g. the birds, the fish).
However, things become a little messy then the image contains multiple elements. For example:

- In the image of the dog, classified as `84.7% white wolf`, `3.2% kuvasz` and `3.0% timber wolf`,
  the gradients highlight the human being as well.
- In the obelisk image (classified as `obelisk`), large portions of the buildings are highlighted by
  both Vanilla and SmoothGrad methods.
- The image containing multiple cat species was classified as
  `43.8% tabby`, `27.2% Egyptian cat`, `6.0% tiger cat` and `2.7% Persian cat`. However,
  the gradients with respect to `tabby` extend to all cats, not only the tabby species.

So it's clear that gradient-based methods are not **class-discriminative**, as not only sections of
the object represented by the output unit are highlighted, but also sections of related objects and
other classes belonging to the set.

## Class Activation Mapping (CAM)
Going in a different direction, a method that focus on class separation was proposed for understanding
convolutional networks, called CAM.

CAM only apply to convolutional networks with a single densely connected softmax layer at its end,
without bias value attached. The TF code for such network looks something like this:
```py
import tensorflow as tf
from tensorflow.keras import Input, Model, layers

C = 1584

backbone = tf.keras.applications.EfficientNetB7(include_top=False)

x = Input((600, 600, 3), name='images')
y = backbone(x)
y = layers.Dense(C, use_bias=False, name='predictions')(y)

network = Model(x, y)
network.compile(..., loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))
```

Let $f$ by a network as proposed above,
$x$ be an input signal (image), $A_{i,j}^k$ the spatial activation signal from
the last convolutional layer in $f$ and
$S_u$ the activation strength of any classifying unit $u$ of the softmax layer:

$$\begin{eqnarray}
S_u(x) &=& \sum_k w_{u,k} \frac{1}{N} \sum_{i,j} A_{i,j}^k \\
       &=& \frac{1}{N}\sum_{i,j} \sum_k w_{u,k} A_{i,j}^k
\end{eqnarray}$$

Where the importance of each spatial unit $A_{i,j}^k$ is $w_{u,k}$.
The definition of importance map follows directly from it:

$$M(f, x, u) = \text{upsample}(\sum_k w_{u,k} A_{i,j}^k)$$

The upsample function refers to the resizing procedure of the map to the input image's original sizes.
Because networks tend to reduce their intermediate signal considerably, with respect to the spatial components
(`EfficientNetB4`, for instance, reduces images $300\times 300\times 3$ to $10\times 10\times 1792$),
a considerable amount of precision is lost in this step.
Hence, this method can only find coarse regions of interest.

From the equations, we can see this signal does not have an upper bound, and
might present arbitrarily large numbers. We can build an attention map by
normalizing it across the spatial components. This normalization also removes the constant $N$ from the eq.

## Grad-CAM

A second article, written by researchers at George Institute of Technology, addresses
the architecture limitations set by CAM. In it, they combine gradient-based saliency
and CAM to replace the $w_{u,k}$ importance factor described above by the differential
of the activating unit with respect to the activating signal:

$$\begin{eqnarray}
M(f, x, u) &=& \text{upsample}(\sum_k w_{u,k} A_{i,j}^k) \\
           &=& \text{upsample}(\sum_k \sum_{l,m}\frac{\partial S_u}{\partial A_{l,m}^k} A_{i,j}^k)
\end{eqnarray}$$

Because the contributions of each kernel in the last convolution are estimated with the gradient information,
we can generalize this attention method to any kind of network. The paper exemplifies it with
a network that combine Conv2D with LSTM layers to generate natural textual descriptors from images.

TensorFlow's implementation of Grad-CAM is pretty simple.
We start by defining the network and feed-forwarding the images through it:
```py
nn = tf.keras.applications.Xception()

x_ = x/127.5 - 1  # Normalize input-signal to (-1, 1), expected by Xception.

logits = nn(x_, training=False)
preds = tf.argmax(logits, axis=1)
probs = tf.nn.softmax(logits)
decoded = model.decode(probs.numpy(), top=2)
```

We then collect the output tensor from the last spatial layer from the model (our signal $A$):
```py
LAST_SPATIAL_LAYER = 'block14_sepconv2_act'

ss = tf.keras.Model(
    inputs=nn.inputs,
    outputs=[nn.output, nn.get_layer(LAST_SPATIAL_LAYER).output],
    name='spatial')
```

And finally define our `gradcam` function:
```py
def activation_gain(y, units):
    return tf.gather(y, units, axis=1, batch_dims=1)

@tf.function
def gradcam(inputs, units):
    with tf.GradientTape(watch_accessed_variables=False) as tape:
        tape.watch(inputs)
        y, z = ss(inputs, training=False)
        loss = activation_gain(y, units)

    grads = tape.gradient(loss, z)
    weights = tf.reduce_mean(grads, axis=(1, 2), keepdims=True)
    maps = tf.reduce_mean(z*weights, axis=-1, keepdims=True)

    # We are not concerned with pixels that negatively contribute
    # to its classification, only pixels that belong to that class.
    maps = tf.nn.relu(maps)
    maps = standardize(maps)
    maps = tf.map_fn(lambda i: tf.image.resize(i, config.data.image_size), maps)

    return loss, maps

def normalize(x):
    x -= tf.reduce_min(x, axis=(1, 2), keepdims=True)
    x /= tf.reduce_max(x, axis=(1, 2), keepdims=True) + 1e-07
    return x
```

Notice we need the arguments `axis=1, batch_dims=1` in `tf.gather` because the `tf.argmax`
operation returned a tensor with a different number of ranks from `y`. A valid alternative
is to remove these args and reshape `preds` as a column vector.

Lastly, we only need to call `gradcam` using our input images and units of interest:
```py
loss, maps = gradcam(x_, preds)
```

{% include figure.html
   src="/assets/images/posts/ml/explaining/cam/grad-cam-1.png"
   alt="Grad-CAM method applied to different input images"
   figcaption="" %}

{% include figure.html
   src="/assets/images/posts/ml/explaining/cam/grad-cam-2.png"
   alt="Grad-CAM method applied to different input images"
   figcaption="Grad-CAM method applied to different input images. The lines (from top to bottom): (a) the original images; (b) the Grad-CAM attention map produced; and (c) the interpolation between the original image and JET colorspace spawned by the attention map." %}


That's it! Hopefully you can re-use this in your own networks! (-:

## References

- Zhou, Bolei, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. "Learning deep features for discriminative localization." IEEE conference on computer vision and pattern recognition (CVPR), pp. 2921-2929. 2016. [1512.04150](https://arxiv.org/pdf/1512.04150.pdf)
- Selvaraju, Ramprasaath R., Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. "Grad-CAM: Visual explanations from deep networks via gradient-based localization." IEEE international conference on computer vision (ICCV), pp. 618-626. 2017. [1610.02391](https://arxiv.org/pdf/1610.02391.pdf)
