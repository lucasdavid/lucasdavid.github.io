---
layout: post
title: Santos Dumont Super Computer
excerpt: "Accessing and using the SDumont infrastructure for Deep Learning research."
toc: true
date: 2021-08-30 10:14:00
lead_image: /assets/images/posts/devops/sdumont/sdumont.jpg
first_p: |-
  This manual details the engagement, access and usage of the Santos Dumont Super Computer.
  This document is profoundly based on the official support manual provided by LNCC,
  as well as my personal experience in the program (hence, it may not perfectly represent all cases).
  Its goal is to present information in a more directed manner for users that share my own profile
  (Deep Learning researchers that prefer the TensorFlow framework and who are familiar with Docker.
tags:
  - DevOps
  - ML
---

<span class="display-6">This</span> post details the engagement, access and usage of the [Santos Dumont Super Computer](https://sdumont.lncc.br/machine.php?pg=machine),
managed by [LNCC](https://www.gov.br/mcti/pt-br/rede-mcti/lncc).
This document is profoundly based on the official [support manual](https://sdumont.lncc.br/support_manual.php) provided by LNCC,
as well as my personal experience in the program (hence, it may not perfectly represent all cases).
Its goal is to present information in a more directed manner for users that share my own profile
(Deep Learning researchers that prefer the [TensorFlow](https://www.tensorflow.org/) framework and who are familiar with [Docker](https://www.docker.com/)).

## SDumont

Santos Dumont is a Brazilian super computer located in the city of Petrópolis, Rio de Janeiro State, Brazil.
A general description of its specs and processing power can be found at [sdumont.lncc.br/machine](https://sdumont.lncc.br/machine.php?pg=machine).

{% include figure.html
   src="/assets/images/posts/devops/sdumont/sdumont.jpg"
   title="External view of Santos Dumont Supercomputer Installations."
   caption='Available at <a href="https://www.gov.br/mcti/pt-br/rede-mcti/lncc/assuntos/noticias/ultimas-noticias-1/novas-informacoes-para-submissao-de-propostas-para-uso-dos-supercomputadores-santos-dumont-lncc-e-lobo-carneiro-coppe-ufrj">gov.br/mcti</a>.'
   classed="w-xl-150" %}

Brazilian scientist and research groups are encouraged to apply for a research project proposal, which will grant
them usage of the infrastructure available.
Furthermore, its processing power can also be used for educational purposes. For this end, educators must first present
a course plan and specify usage details.

## Project Proposal
Engagement process starts at this [website](https://sdumont.lncc.br/call.php?pg=call#).
It states proposals will be evaluated up to November 27th, 2021.
To be considered, you must fill in [this application](https://sdumont.lncc.br/formularios/SDumont_Formulario_de_Proposta.pdf)
and submit it through the [JEMS system](https://jems.sbc.org.br/home.cgi?c=3557).

## First Access and Setup
Once a proposal is accepted, a welcoming e-mail is sent from <jems@sbc.org.br> to all authors.
The e-mail contains details and comments from reviewers regarding the project proposal, as well as grading score per evaluation category.

### User and Project Registration
The welcoming e-amil will ask the project members to fill in two two forms:

- Project Coordinator Form: should be filled in by the project coordinator,
  which formalizes the project scope (by repeating what was written in the proposal or by updating it with justifiable notice)
  and lists all members involved in the project. [link](http://sdumont.lncc.br/formularios/formulario_sdumont_coordenador.docx)
- Project User Form: should be filled by each member that was listed in the Project Coordinator Form (one form per member).
  [link](http://sdumont.lncc.br/formularios/formulario_sdumont_usuario.docx)

Identification information (such as "Registro Geral", or R.G.) must be provided in both forms, which must be printed and signed
(or digitally signed).
The forms, as well as the scans of ID documents for each project member and coordinator, must be e-mailed to 
helpdesk-sdumont@lncc.br and sdumont@lncc.br. There is a timeframe of 30 days to complete this stage of registration.

I used the following template for the e-mail:

```
Bom dia.
Como requerido, seguem anexos os formulários de cadastro e cópias dos documentos.

Projeto: {PROJECT_TITLE_AS_WRITTEN_IN_THE_PROPOSAL}
Proposta: #{PROJECT_ID_NUMBER_IN_JEMS}

Membros:
* Nome: {NAME_SURNAME} (coordenador)
  E-mail: {EMAIL@DOMAIN}
  Instituição: {INSTITUTE, UNIVERSITY}
* Nome: {NAME SURNAME}
  E-mail: {EMAIL@DOMAIN}
  Instituição: {INSTITUTE, UNIVERSITY}
*  ...
*  ...
*  ...

Atenciosamente,
```

You will receive the following confirmation e-mail:

```
FROM: helpdesk-sdumont@lncc.br
SUBJECT: Re: Formulários de cadastro SDumont - ID {ID_NUMBER}

Prezado {NAME},

Registramos os chamados abaixo para criação do projeto e contas de usuários:

Chamado - Abertura de conta de projeto SDumont - {PROJECT_CODE} - ID {PROJECT_ID}
Chamado - Abertura de conta de usuário SDumont - {NAME} - ID {USER_ID}
Chamado - Abertura de conta de usuário SDumont - {NAME} - ID {USER_ID}
...
```

Wait for a day. Each member listed by the Coordinator will receive their own registration confirmation e-mail.
You will find your name and ID detailed in the confirmation e-mail's subject.
Your temporary password, however, is not stored in the e-mail (or anywhere else).
You must call LNCC on the phone number <phone>+55 (24) 98839-7182</phone> and tell them you have just received the confirmation e-mail.
Inform your name and ID number and they will verbally inform you of your password.

Configure and open the VPN tunnel (using the instructions in the e-mail and the credentials provided on the call).
I use linux and opted for the graphic VPN interface, so the following packages are necessary:
```shell
sudo apt-get install vpnc network-manager-vpnc network-manager-vpnc-gnome
```

Once it is opened, you can ssh into the buttler host:
```shell
ssh USERNAME@login.sdumont.lncc.br
password: ******
```

Use your temporary password. Once you are logged, you will be asked to change it.
Once this is done, you will be disconnected from the VPN.
Update its settings to reflect the new pass, if necessary, and open the tunnel once again.
SSH into the butler once more and you will now be connected to SDumont!

## Usage

### Storage

Two storage partitions are available [[ref]](https://sdumont.lncc.br/alloc.php?pg=alloc#):

Name | Capacity | Location | Description
-----|----------|------------
Scratch | 25 TB | `/scratch/{PROJECT}/{USER}` | input, transient and output data
Home | 5 TB | `/prj/{PROJECT}/{USER}` | source-code, libraries
{:class="dataframe table table-hover"}

\* Scratch is 50 TB in Premium tier projects.

Files in scratch are erased after 60 days without being updated, and should be backed up in Home.
Nodes cannot access files in Home, so every code file/dataset/archive must be copied to the scratch partition:
```shell
rsync -rUv $HOME/experiment-files/ $SCRATCH/experiment-files/
rsync -rUv $SCRATCH/logs/experiment-results/ $HOME/logs/experiment-results/
```

### GPUs

I listed below the most interesting GPU queues for Machine Learning, as well as their specs.

name         | Wall time (hours) | CPU | Cores | Memory | GPU | Cost
-------------|-------------------|--------------------------- 
nvidia_dev   |              0.33 |	 2 |    24 | 64     | 2  K40 | 1.5
nvidia_small |                 1 |   2 |    24 | 64     | 2  K40 | 1.5
nvidia_long  | 	   744 (31 days) |	 2 |    24 | 64     | 2  K40 | 1.5
nvidia_scal  |                18 | 102 |  1224 | 6528   | 102  K40 | 1.5
nvidia       |                48 |  42 |   504 | 2688   | 42  K40 | 1.5
sequana_gpu  |                96 |   2 |    48 | 384    | 4 V100 | 1.5
sequana_gpu_dev |           0.33 |   2 |    48 | 384    | 4 V100 | 1.5   
sequana_gpu_long | 744 (31 days) |   2 |    48 | 384    | 4 V100 | 1.5
gdl          |                48 |   2 |    40 | 384    | 8 V100 | 2.0
{:class="dataframe table table-hover"}


### Submitting Jobs
#### NVIDIA Queues
Should be used for the regular experiments (two GPUs with 12 GB are available).
This nodes can be interactively accessed through the following code:

```shell
# Access
$ ssh {USER}@login.sdumont.lncc.br
salloc --nodes=1 -p {QUEUE} -J {NAME} --exclusive

salloc: Pending job allocation {JOB_ID}
salloc: job {JOB_ID} queued and waiting for resources
salloc: job {JOB_ID} has been allocated resources
salloc: Granted job allocation {JOB_ID}

$ squeue -j {JOB_ID}
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
          {JOB_ID}   {QUEUE}   {NAME}   {USER}  R       0:21      1 {NODE_ID}

$ ssh {NODE_ID}
```

Now go to this [tensorflow.org/install](https://www.tensorflow.org/install/source#gpu) and check which
CUDA and python should you be using for your specific tensorflow version.

For example, I'm using `tensorflow-gpu==2.6.0`, so I would use python>3.7 and CUDA>11.2:
```shell
# Load appropriate libraries
$ module load python/3.8.2 cudnn/8.1
$ pip install tensorflow-gpu tensorflow-datasets tensorflow-addons

# Experiment run
$ python script.py

$ exit  # Exit {NODE_ID}
$ exit  # Dispose {JOB_ID}
$ exit  # Exit SSH
```


#### `gdl` Queue
Sequana queue, for very deep models (and usage cost of 2.0 UAs).

##### Ex 1
Iteractively:
```shell
salloc --nodes=1 -p gdl -J GDL-teste --exclusive

salloc: Granted job allocation 123456

#verificar quais nós foram alocados para o job
squeue -j 123456
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
            123456       gdl     bash usuario1  R       5:28      1 sdumont4000

#acessa o nó
ssh sdumont4000

#carrega o módulo e executa a aplicação 
[usuario1@sdumont4000 ~]$ cd /scratch/projeto/usuario1/teste-gdl
[usuario1@sdumont4000 teste-gdl]$ module load deepl/deeplearn-py3.7
[usuario1@sdumont4000 teste-gdl]$ python script.py 

#encerra a conexão com o nó
[usuario1@sdumont4000 teste-gdl]$ exit

#encerra a sessão interativa e termina o job
exit
salloc: Relinquishing job allocation 123456
```

##### Example 2

Create a job file `experiment.srm`:
```shell
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --ntasks=1
#SBATCH -p gdl
#SBATCH -J GDL-teste-script
#SBATCH --exclusive

#Exibe os nos alocados para o Job
echo $SLURM_JOB_NODELIST
nodeset -e $SLURM_JOB_NODELIST

cd $SLURM_SUBMIT_DIR

#Configura o módulo de Deep Learning
module load deepl/deeplearn-py3.7

#acessa o diretório onde o script está localizado 
cd /scratch/projeto/usuario1/teste-gdl

#executa o script
python script.py  
```

and then run it:
```
sbatch experiment.srm
```
