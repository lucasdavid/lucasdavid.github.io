<!DOCTYPE html>
<html lang="en">
<head>
  <title>Observing Optimizer Behavior over Search Surfaces – Lucas David</title>
  <meta charset="utf-8" />
<meta content='text/html; charset=utf-8' http-equiv='Content-Type'>
<meta http-equiv='X-UA-Compatible' content='IE=edge'>
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="image_src" type="image/png" href="img_path" />


<meta name="description" content="Visualizing the training process over various optimization surfaces." />
<meta property="og:description" content="Visualizing the training process over various optimization surfaces." />
<meta property="og:image" content="" />

<meta name="author" content="Lucas David" />


<meta property="og:title" content="Observing Optimizer Behavior over Search Surfaces" />
<meta property="twitter:title" content="Observing Optimizer Behavior over Search Surfaces" />


<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<link rel="alternate" type="application/rss+xml" title="Lucas David - my personal website/blog"
      href="/feed.xml" />

  
  
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-LG7FZ8VCHM"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'G-LG7FZ8VCHM');
	</script>


  
  <!--[if lt IE 9]>
  <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="/style.css" />

</head>
<body>
  <nav id="mainNav" class="navbar navbar-light bg-white navbar-expand-lg d-print-none border-bottom border-light ">
  <div class="container-xl">
    <button class="navbar-toggler rounded-0 border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navbarTogglerDemo01"
      aria-controls="navbarTogglerDemo01" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <a class="navbar-brand fw-bold text-decoration-none p-1" href="/">Lucas David</a>

    <div class="collapse navbar-collapse" id="navbarTogglerDemo01">

      <ul class="navbar-nav me-auto mb-2 mb-lg-0">
      </ul>
      <span class="navbar-text navbar-excerpt">
        Observing Optimizer Behavior over Search Surfaces
      </span>
      <span class="navbar-text font-small ms-2 me-2 sr-none" aria-hidden="true">•</span>
      <ul class="navbar-nav mb-2 mb-lg-0 font-small">
        <li class="nav-item"><a href="/" class="nav-link fw-bold link-dark fs-6">Home</a></li>
        <li class="nav-item"><a href="/blog" class="nav-link fw-bold link-dark fs-6">Blog</a></li>
        <li class="nav-item"><a href="/publications" class="nav-link fw-bold link-dark fs-6">Publications</a></li>

        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle fw-bold link-dark fs-6" href="#" id="nbd-links-social" role="button"
            data-bs-toggle="dropdown" aria-expanded="false">
            Social
          </a>
          <ul class="dropdown-menu font-small dropdown-menu-end" aria-labelledby="nbd-links-social">
            <li><a class="dropdown-item text-decoration-none" target="_blank"
                href="https://github.com/lucasdavid">
                <i aria-label="GitHub" class="bi bi-github link-dark sr-none"></i>
                Github</a></li>
            <li><a class="dropdown-item text-decoration-none" target="_blank"
                href="https://www.linkedin.com/in/ld7">
                <i class="bi bi-linkedin text-primary sr-none"></i>
                Linkedin</a></li>

            <li itemscope itemtype="https://schema.org/Person">
              <a itemprop="sameAs" content="https://orcid.org/0000-0002-8793-7300" href="https://orcid.org/0000-0002-8793-7300"
                  target="orcid.widget"
                  rel="me noopener noreferrer"
                  class="dropdown-item text-decoration-none"
                >
                <img src="/assets/images/infra/32px-ORCID_iD.webp" alt="ORCID logo" class="sr-none" style="margin-bottom: 5px; width: 16px;">
                ORCID</a>
            </li>

            <li><a class="dropdown-item text-decoration-none" target="_blank"
                href="http://stackoverflow.com/users/2429640/lucasdavid">
                <i class="bi bi-code-slash link-dark sr-none"></i>
                Stackoverflow</a></li>
            <li><a class="dropdown-item text-decoration-none" target="_blank"
                href="https://youtube.com/channel/UC7IWeKUy4OSlC5ripcQGD6Q">
                <i class="bi bi-youtube text-danger sr-none"></i>
                Youtube</a></li>
          </ul>
        </li>
      </ul>
    </div>
  </div>
</nav>

  <div class="empty-v-space d-none d-xl-block" style="margin-bottom: 5vh"></div>
</div>
<div class="container-fluid mt-4">
  <div class="row">
    <div class="col-12 col-xl-3 col-xxl-2 offset-xxl-1">
      <div id="sidebar" class="">
  <div class="text-center">
    <a href="/" title="Home">
      <img src="/assets/images/infra/aloy-100.png" alt="Aloy, a character from Horizon Zero Dawn."
        class="img-fluid rounded-circle" style="width:100px" />
    </a>
    <p class="pt-2 font-small">
      
        <a href="mailto:mb37410l3@mozmail.com" class="text-decoration-none fw-bold">mb37410l3@mozmail.com</a>
      
    </p>
  </div>

  
</div>

    </div>
    <div id="table-of-contents-container-r" class="col-12 col-xl-3 col-xxl-2 order-xl-2">
      
      <div style="z-index: 0; font-size:0.8rem;">
        <div id="table-of-contents" class="font-small">
  <p class="border-bottom"><strong>Summary</strong></p>
  <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#visualizing-surface-equations">Visualizing Surface Equations</a>
<ul>
<li class="toc-entry toc-h3"><a href="#descriptive-optimization-functions">Descriptive Optimization Functions</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#modern-optimization-methods">Modern Optimization Methods</a>
<ul>
<li class="toc-entry toc-h3"><a href="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
<li class="toc-entry toc-h3"><a href="#momentum">Momentum</a></li>
<li class="toc-entry toc-h3"><a href="#rmsprob">RMSProb</a></li>
<li class="toc-entry toc-h3"><a href="#adagrad">AdaGrad</a></li>
<li class="toc-entry toc-h3"><a href="#adam">Adam</a></li>
<li class="toc-entry toc-h3"><a href="#lion">Lion</a></li>
<li class="toc-entry toc-h3"><a href="#lars--lamb">LARS &amp; LAMB</a></li>
<li class="toc-entry toc-h3"><a href="#ftrl">FTRL</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#surface-transversing-with-modern-optimizers">Surface Transversing with Modern Optimizers</a></li>
<li class="toc-entry toc-h2"><a href="#final-considerations">Final Considerations</a></li>
<li class="toc-entry toc-h2"><a href="#references">References</a></li>
</ul>
</div>

      </div>
      
    </div>
    <div class="col-12 col-xl-6">
      <article class="post mb-4">
        <header class="">
          <h1 id="postTitle" class="fw-bold">Observing Optimizer Behavior over Search Surfaces</h1>
          <p class="right-align mb-2 text-muted fs-5">
            Visualizing the training process over various optimization surfaces. <em>— November 12, 2024</em>
          </p>
          <div class="mb-4">
            <span class="badges-container">
  
    <a href="/blog/tag/ml"
       class="btn badge rounded-pill btn-dark text-bg-dark text-decoration-none"
       style="font-size: 0.8em;"
       type="button"
       role="button"
       >ML</a>
  
    <a href="/blog/tag/optimization"
       class="btn badge rounded-pill btn-dark text-bg-dark text-decoration-none"
       style="font-size: 0.8em;"
       type="button"
       role="button"
       >Optimization</a>
  
    <a href="/blog/tag/visualization"
       class="btn badge rounded-pill btn-dark text-bg-dark text-decoration-none"
       style="font-size: 0.8em;"
       type="button"
       role="button"
       >Visualization</a>
  
</span>

          </div>
        </header>
        <div class="article-content" style="margin-top: 4rem;">
          <p><span class="fs-1" style="line-height:0">T</span>he goal of this post is to
give the reader an introduction over the existing optimization algorithms,
to illustrate and compare their behavior over the optimization surfaces,
which ultimately translates to their effectiveness over AI tasks.</p>

<p>Training a Machine Learning model consists of traverse an optimization surface,
seeking the best solution: the configuration of parameters $\bm θ^\star$
associated to the lowest error or best fit.
It would be interesting to have a have a visual representation of this process, in which
problems such as local minima, noise and overshooting are explicit and better understood.</p>

<p>A <strong>Machine Learning model is a parameterized predictive function</strong> that associates every
input sample to a desired output signal.
Formally, let $\bm{x}_i\in \mathbb{R}^f$ be an input sample in
the dataset $\mathbf{X} = \lbrace\bm{x}_0, \bm{x}_1, \ldots, \bm{x}_n\rbrace$, then
$f(\bm{x}_i,\bm θ_t) = \bm p_i\in\mathbb{R}^c$ is a predictive function parameterized
by $\bm θ_t \in \mathbb{R}^m$ at a given training step $t$.</p>

<p>If the problem is supervised, each sample $\bm{x}_i$ is paired with an expected value $\bm{y}_i\in \mathbf{Y}$, often a product of manual and laborious annotation by specialists, and referred to as “ground-truth”.
A loss function $\mathcal{L}(\mathbf{X}, \mathbf{Y}, \bm θ_t)$ is also adopted, describing how far off are the predictions outputted by the model parameterized by $\bm θ_t$.
Examples of such functions can be checked in my post about <a href="/blog/machine-learning/crossentropy-and-logits">cross entropy and logit</a> functions.</p>

<div class="w-xl-auto ms-xl-n4 bg-light pb-2 mt-4 mb-4">
<div class="container-fluid">
<div class="row">
<div class="col-12 col-xl-6 offset-xl-3 mt-2">

        <h6 id="linear-regression-example">Linear Regression Example</h6>

        <p>Linear regression is a type of linear model, and it is characterized as:</p>

\[\begin{align}\begin{split}
\bm x_i &amp;\in \mathbf{X}, \bm p_i \in \mathbb{R}^c, \bm θ_t\in \mathbb{R}^{f+1}\\
\bm p_i &amp;= f(\bm x_i, \bm θ_t)
        = \sigma\left(θ_t^\intercal \cdot \left[\bm x_i \mid 1 \right]\right)
        = \sigma\left(\sum_k^f θ_k \left[\bm x_i \mid 1 \right]_k\right) \\
        &amp;= \sigma\left(θ_{t,0}\bm x_{i,0} + θ_{t,1}\bm x_{i,1} + \ldots +
           θ_{t,f-1}\bm x_{i,f-1} + θ_{t,f}\right)
\end{split}\end{align}\]

        <p>where $\left[ \bm x_i \mid 1 \right]$ is concatenation between $\bm x_i$ and $1$, and $\sigma$
is the <a href="https://en.wikipedia.org/wiki/Sigmoid_function" target="_blank">sigmoid function</a>.</p>

        <p>Concatenating the input data with 1 is a simplification that allows us to express all parameters
(including the bias factor $\bm θ_{t,f}$) in a single vector $\bm θ_t$.</p>

        <p>Training a LR model over a dataset $\mathbf{X}$ means to find a set of parameters $\bm θ^\star$
such that $\mathcal{L}$ assumes its minimum value. For unconstrained quadratic optimization, where
$\mathcal{L}(\mathbf{X}, \mathbf{Y}, \bm θ_t) = \frac{1}{2}\sum_i\left(f(x_i,\bm θ_t) - y_i\right)^2$ only
one point exists such that $\nabla_{\bm θ^\star}\mathcal{L} = 0$.
In this case, the solution can be found by solving a linear equation system (or matrix inversion):
$\bm\theta_\star = \left[\mathbf{X} \mid 1\right]^{-1}\mathbf{Y}$).
As for unconstrained non-convex minimization, iterative solutions (e.g., Gradient Descent) are required.</p>

      </div>
</div>
</div>
</div>

<div style="" class="w-100 w-md-50 float-md-end ms-2">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/surfaces/s3.png" alt="Quadric Equation Surface:" class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-center">
    <strong class="title">Quadric Equation Surface:</strong>

    $f(x, y) = x^2-y^2 -xy$.
  </figcaption>


  </figure>
</div>

<p>As $\bm θ_t$ is contained in the $\mathbb{R}^m$ Euclidean space, it can represented as a point.
Moreover, all possible points $\left[\bm θ_t \mid \mathcal{L}(\bm{X}, \bm{Y}, \bm θ_t)\right]$
(obtained by concatenating the parameters with their associated <em>loss</em> values) form a surface in $\mathbb{R}^{m+1}$.</p>

<p>If the functions $f$ and $\mathcal{L}$ are continuously differentiable, then the surface spawned
by them is also continuously differentiable. This ensures that the derivative is defined and finite
in all points in the domain, and that the Gradient vector can be calculated and used to find the
direction of update $-\nabla\mathcal{L}$ for each parameter of the model.</p>

<p>However, many modern models violate this condition by containing non-continuous or non-differentiable functions,
such as the Rectified Linear Unit $\text{ReLU}(x) = \max(x, 0)$ function. <a class="citation" href="#agarap2018deep">[1]</a>
This isn’t such a big problem, as non-continuous functions employed are always sub-differentiable.
I.e., their derivatives can be approximated by known convex functions.
As automatic differentiation frameworks (e.g., PyTorch, Tensorflow) often work with pre-defined function
and their respective derivatives,
non-continuous functions will have one of these convex functions registered as their derivatives.</p>

<p>Model complexity can also severely twist the optimization manifold, resulting in the distancing
between the real value and the linear estimation expressed in the gradient. In such cases, it’s
common to transverse the search space in smaller steps, reducing the inherit error of optimizing
a complex non-linear function through its first order approximation, at the cost of more iterations
and higher risk of being stuck at local minima. In practice, however, the optimization surfaces
are usually well-behaved (somewhat smooth and simple) if certain conditions are
met: <a class="citation" href="#li2018visualizing">[2]</a> the model has few parameters, strong regularization is employed,
the underlying function that represents data is also well-behaved.</p>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseSetup" aria-controls="collapseSetup" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show setup code</button>
</div>

<div class="language-python collapse highlighter-rouge" id="collapseSetup"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">print_figure_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="s">'bbox_inches'</span><span class="p">:</span> <span class="bp">None</span><span class="p">}</span>  <span class="c1"># fix for animations with `xlim`.
</span>
<span class="n">FIG_SIZES</span> <span class="o">=</span> <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>
<span class="n">VIEW</span> <span class="o">=</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">70</span><span class="p">)</span>

<span class="n">XY_RANGE</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">MAX_LOSS_VALUE</span> <span class="o">=</span> <span class="mf">999.99999</span>
<span class="n">MARGIN</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.animation</span> <span class="k">as</span> <span class="n">mpl_animation</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">sns</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s">'whitegrid'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">.</span><span class="n">update</span><span class="p">({</span><span class="s">'xtick.color'</span><span class="p">:</span><span class="s">'#555'</span><span class="p">,</span> <span class="s">'ytick.color'</span><span class="p">:</span><span class="s">'#555'</span><span class="p">,</span> <span class="s">"animation.embed_limit"</span><span class="p">:</span> <span class="mf">10e9</span><span class="p">})</span>

<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"animation.html"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"jshtml"</span>


<span class="k">def</span> <span class="nf">show_sm_surface</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">70</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">viridis</span><span class="p">):</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">y</span><span class="p">[...,</span> <span class="n">sm</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">view_init</span><span class="p">(</span><span class="o">*</span><span class="n">view</span><span class="p">)</span>

  <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"$x_2$"</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="sa">f</span><span class="s">"$f$"</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="n">pane</span><span class="p">.</span><span class="n">fill</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="n">pane</span><span class="p">.</span><span class="n">fill</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">zaxis</span><span class="p">.</span><span class="n">pane</span><span class="p">.</span><span class="n">fill</span> <span class="o">=</span> <span class="bp">False</span>

  <span class="k">if</span> <span class="n">title</span><span class="p">:</span> <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">y</span><span class="o">=-</span><span class="mf">0.05</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">ax</span>

<span class="k">def</span> <span class="nf">show_surface</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"cool"</span><span class="p">):</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">show_sm_surface</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">],</span> <span class="n">sm</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s">'(a)'</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="n">view</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>

  <span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="n">_axinfo</span><span class="p">[</span><span class="s">"grid"</span><span class="p">].</span><span class="n">update</span><span class="p">({</span><span class="s">"linestyle"</span><span class="p">:</span><span class="s">"--"</span><span class="p">})</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="n">_axinfo</span><span class="p">[</span><span class="s">"grid"</span><span class="p">].</span><span class="n">update</span><span class="p">({</span><span class="s">"linestyle"</span><span class="p">:</span><span class="s">"--"</span><span class="p">})</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">zaxis</span><span class="p">.</span><span class="n">_axinfo</span><span class="p">[</span><span class="s">"grid"</span><span class="p">].</span><span class="n">update</span><span class="p">({</span><span class="s">"linestyle"</span><span class="p">:</span><span class="s">"--"</span><span class="p">})</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

  <span class="k">return</span> <span class="n">ax</span>


<span class="c1">#@title Plotting &amp; Animation Functions
</span>
<span class="k">def</span> <span class="nf">plot_surface_and_steps</span><span class="p">(</span>
    <span class="n">opt_points</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span>
    <span class="n">view</span><span class="o">=</span><span class="n">VIEW</span><span class="p">,</span>
    <span class="n">colors</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="n">zlim</span><span class="o">=</span><span class="s">"auto"</span><span class="p">,</span>
    <span class="n">duration</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">legend_title</span><span class="o">=</span><span class="s">"Optimizers &amp; losses"</span><span class="p">,</span>
    <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">animated</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">figsize</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
<span class="p">):</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span> <span class="ow">or</span> <span class="n">FIG_SIZES</span><span class="p">)</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span> <span class="o">=</span> <span class="s">'3d'</span><span class="p">)</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">show_surface</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">f</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="n">view</span><span class="p">)</span>

  <span class="n">p_point</span><span class="p">,</span> <span class="n">p_trace</span><span class="p">,</span> <span class="n">p_texts</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{},</span> <span class="p">{}</span>

  <span class="k">if</span> <span class="n">colors</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s">"Set2"</span><span class="p">))</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">colors</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">opt_points</span><span class="p">))]</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">opt_points</span><span class="p">,</span> <span class="n">colors</span><span class="p">))</span>

  <span class="n">p</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">animated</span> <span class="k">else</span> <span class="bp">None</span>
  <span class="n">last</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">animated</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>

  <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
    <span class="n">c_text</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">e</span><span class="o">/</span><span class="mi">2</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">c</span><span class="p">)</span>

    <span class="n">p_point</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">points</span><span class="p">[:</span><span class="n">p</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[:</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">label</span><span class="p">][:</span><span class="n">p</span><span class="p">],</span> <span class="s">'.'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">y</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">last</span><span class="p">]:.</span><span class="mi">5</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">p_trace</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">points</span><span class="p">[:</span><span class="n">p</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[:</span><span class="n">p</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">label</span><span class="p">][:</span><span class="n">p</span><span class="p">],</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">c</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">p_texts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">points</span><span class="p">[</span><span class="n">last</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[</span><span class="n">last</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">last</span><span class="p">],</span> <span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">y</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">last</span><span class="p">]:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">c_text</span><span class="p">)</span>

  <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="n">XY_RANGE</span><span class="p">,</span> <span class="n">XY_RANGE</span><span class="p">))</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="n">XY_RANGE</span><span class="p">,</span> <span class="n">XY_RANGE</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">legend</span><span class="p">:</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s">'family'</span><span class="p">:</span> <span class="s">'monospace'</span><span class="p">},</span> <span class="n">title</span><span class="o">=</span><span class="n">legend_title</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">L</span> <span class="o">=</span> <span class="bp">None</span>

  <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

  <span class="c1"># if title: plt.suptitle(title)
</span>  <span class="k">if</span> <span class="n">zlim</span><span class="p">:</span> <span class="n">ax</span><span class="p">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="n">zlim</span><span class="p">)</span>

  <span class="k">if</span> <span class="ow">not</span> <span class="n">animated</span><span class="p">:</span>
    <span class="n">ani</span> <span class="o">=</span> <span class="bp">None</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">_animation_update</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
      <span class="c1"># l_shift = max([t.get_window_extent().width for t in L.get_texts()])
</span>      <span class="n">size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">)</span>

      <span class="n">texts</span> <span class="o">=</span> <span class="n">L</span><span class="p">.</span><span class="n">get_texts</span><span class="p">()</span> <span class="k">if</span> <span class="n">L</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="p">[</span><span class="bp">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">]</span>
      <span class="k">for</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">points</span><span class="p">),</span> <span class="n">l_text</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">texts</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">num</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
          <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">num</span><span class="p">],</span> <span class="n">MAX_LOSS_VALUE</span><span class="p">),</span> <span class="o">-</span><span class="n">MAX_LOSS_VALUE</span><span class="p">)</span>

        <span class="n">p_point</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_data</span><span class="p">(</span><span class="n">points</span><span class="p">[</span><span class="n">num</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="n">num</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">].</span><span class="n">T</span><span class="p">)</span>
        <span class="n">p_point</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_3d_properties</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">num</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="n">num</span><span class="p">])</span>

        <span class="n">p_texts</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">loss</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="n">p_texts</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_x</span><span class="p">(</span><span class="n">points</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="mi">0</span><span class="p">]);</span> <span class="n">p_texts</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_y</span><span class="p">(</span><span class="n">points</span><span class="p">[</span><span class="n">num</span><span class="p">,</span> <span class="mi">1</span><span class="p">]);</span> <span class="n">p_texts</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_z</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">label</span><span class="p">][</span><span class="n">num</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">num</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
          <span class="n">p_trace</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_data</span><span class="p">(</span><span class="n">points</span><span class="p">[:</span><span class="n">num</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">].</span><span class="n">T</span><span class="p">)</span>
          <span class="n">p_trace</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_3d_properties</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">label</span><span class="p">][:</span><span class="n">num</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">l_text</span><span class="p">:</span> <span class="n">l_text</span><span class="p">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">label</span><span class="p">.</span><span class="n">ljust</span><span class="p">(</span><span class="n">size</span><span class="p">)</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">loss</span><span class="p">:</span><span class="mf">10.5</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="n">first_points</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">opt_points</span><span class="p">.</span><span class="n">values</span><span class="p">()))</span>
    <span class="n">frames</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">first_points</span><span class="p">)</span>

    <span class="n">ani</span> <span class="o">=</span> <span class="n">mpl_animation</span><span class="p">.</span><span class="n">FuncAnimation</span><span class="p">(</span>
      <span class="n">fig</span><span class="p">,</span> <span class="n">_animation_update</span><span class="p">,</span> <span class="n">frames</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="n">duration</span><span class="o">/</span><span class="n">frames</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">p_point</span><span class="p">,</span> <span class="n">p_trace</span><span class="p">,</span> <span class="n">p_texts</span><span class="p">),</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span>


<span class="k">def</span> <span class="nf">plot_loss_in_time</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">zlim</span><span class="o">=</span><span class="s">"auto"</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mf">10000.</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span> <span class="ow">or</span> <span class="n">FIG_SIZES</span><span class="p">)</span>
  <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>

  <span class="n">p_trace</span><span class="p">,</span> <span class="n">p_texts</span> <span class="o">=</span> <span class="p">{},</span> <span class="p">{}</span>

  <span class="k">if</span> <span class="n">colors</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">(</span><span class="s">"Set2"</span><span class="p">))</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="o">%</span><span class="nb">len</span><span class="p">(</span><span class="n">colors</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">opt_points</span><span class="p">))]</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">opt_points</span><span class="p">,</span> <span class="n">colors</span><span class="p">))</span>

  <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">loss</span> <span class="ow">in</span> <span class="n">y</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">p_trace</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss</span><span class="p">[:</span><span class="mi">1</span><span class="p">].</span><span class="n">clip</span><span class="p">(</span><span class="o">-</span><span class="n">MAX_LOSS_VALUE</span><span class="p">,</span> <span class="n">MAX_LOSS_VALUE</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">label</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">p_texts</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">loss</span><span class="p">[:</span><span class="mi">1</span><span class="p">].</span><span class="n">clip</span><span class="p">(</span><span class="o">-</span><span class="n">MAX_LOSS_VALUE</span><span class="p">,</span> <span class="n">MAX_LOSS_VALUE</span><span class="p">),</span> <span class="n">label</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">label</span><span class="p">])</span>

  <span class="n">first_points</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">opt_points</span><span class="p">.</span><span class="n">values</span><span class="p">()))</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="mi">0</span><span class="o">-</span><span class="n">MARGIN</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">first_points</span><span class="p">)</span><span class="o">+</span><span class="n">MARGIN</span><span class="p">))</span>

  <span class="k">if</span> <span class="n">zlim</span> <span class="o">==</span> <span class="s">"auto"</span><span class="p">:</span>
    <span class="n">all_points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">()),</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">all_points</span> <span class="o">=</span> <span class="n">all_points</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">all_points</span><span class="p">)]</span>
    <span class="n">zlim</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">all_points</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
    <span class="n">zlim</span> <span class="o">=</span> <span class="p">(</span><span class="n">zlim</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">MARGIN</span><span class="p">,</span> <span class="n">zlim</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">MARGIN</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">zlim</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">zlim</span><span class="p">)</span>

  <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s">"--"</span><span class="p">)</span>
  <span class="n">L</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">prop</span><span class="o">=</span><span class="p">{</span><span class="s">'family'</span><span class="p">:</span> <span class="s">'monospace'</span><span class="p">},</span> <span class="n">title</span><span class="o">=</span><span class="s">"Losses"</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

  <span class="k">def</span> <span class="nf">_animation_update</span><span class="p">(</span><span class="n">num</span><span class="p">):</span>
    <span class="n">label_size</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">num</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">points</span><span class="p">),</span> <span class="n">l_text</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">L</span><span class="p">.</span><span class="n">get_texts</span><span class="p">()):</span>
      <span class="k">if</span> <span class="n">num</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
        <span class="n">num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span>

      <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">((</span>
          <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num</span><span class="p">),</span>
          <span class="n">points</span><span class="p">[:</span><span class="n">num</span><span class="p">].</span><span class="n">clip</span><span class="p">(</span><span class="o">-</span><span class="n">MAX_LOSS_VALUE</span><span class="p">,</span> <span class="n">MAX_LOSS_VALUE</span><span class="p">),</span>
      <span class="p">)).</span><span class="n">T</span>

      <span class="n">loss</span> <span class="o">=</span> <span class="n">points</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
      <span class="n">p_trace</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_data</span><span class="p">(</span><span class="n">points</span><span class="p">[:</span><span class="n">num</span><span class="p">].</span><span class="n">T</span><span class="p">)</span>
      <span class="n">p_texts</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">loss</span><span class="p">:.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
      <span class="n">p_texts</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_x</span><span class="p">(</span><span class="n">points</span><span class="p">[</span><span class="n">num</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
      <span class="n">p_texts</span><span class="p">[</span><span class="n">label</span><span class="p">].</span><span class="n">set_y</span><span class="p">(</span><span class="n">points</span><span class="p">[</span><span class="n">num</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
      <span class="n">l_text</span><span class="p">.</span><span class="n">set_text</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">label</span><span class="p">.</span><span class="n">ljust</span><span class="p">(</span><span class="n">label_size</span><span class="p">)</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">loss</span><span class="p">:</span><span class="mf">10.5</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

  <span class="n">frames</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">first_points</span><span class="p">)</span>

  <span class="n">ani</span> <span class="o">=</span> <span class="n">mpl_animation</span><span class="p">.</span><span class="n">FuncAnimation</span><span class="p">(</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">_animation_update</span><span class="p">,</span> <span class="n">frames</span><span class="p">,</span> <span class="n">interval</span><span class="o">=</span><span class="n">duration</span><span class="o">/</span><span class="n">frames</span><span class="p">,</span> <span class="n">blit</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ani</span>
</code></pre></div></div>

<h2 id="visualizing-surface-equations">Visualizing Surface Equations</h2>

<p>To simulate our problem, we take all points in the 2D mesh grid <code class="language-plaintext highlighter-rouge">(-10, 10)</code> (spaced by $0.1$),
as well as two noise vectors that simulate fine and coarse-grained noises, respectively:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="n">XY_RANGE</span><span class="p">,</span> <span class="n">XY_RANGE</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">stack</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="n">shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>

<span class="n">r</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">((</span><span class="n">a</span> <span class="o">//</span> <span class="n">B</span><span class="p">,</span> <span class="n">b</span> <span class="o">//</span> <span class="n">B</span><span class="p">))</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">resize</span><span class="p">(</span><span class="n">r</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">],</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">method</span><span class="o">=</span><span class="s">"bilinear"</span><span class="p">)[...,</span> <span class="mi">0</span><span class="p">]</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
<span class="n">c</span> <span class="o">*=</span> <span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">/</span> <span class="mf">100.0</span>
</code></pre></div></div>

<p>The vectors <code class="language-plaintext highlighter-rouge">c</code> and <code class="language-plaintext highlighter-rouge">r</code> contain noise values for each of the $200\times 200$ points
in the mesh. However, we can transverse the space and end up in an intermediate location,
between two points represented within the mesh.
So we also define a <code class="language-plaintext highlighter-rouge">approximate_positional_noise</code> function, which determines the noise
for a given list of points <code class="language-plaintext highlighter-rouge">z</code> by matching these points to their closest neighbors in the mesh:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">global</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span>

  <span class="n">s</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
  <span class="c1"># Get index (and corresponding noise) for the closest to the input point.
</span>  <span class="n">indices</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">((</span><span class="n">z</span><span class="o">+</span><span class="n">XY_RANGE</span><span class="p">)</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">XY_RANGE</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">indices</span><span class="p">),</span> <span class="n">tf</span><span class="p">.</span><span class="n">gather_nd</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</code></pre></div></div>

<p>Tensorflow stores all operations performed under a <code>tf.GradientTape</code> context,
allowing us to automatically compute their Gradient vectors with respect to
any variables by simply calling <code>tf.gradient</code>.
The direction of highest accent, for a function $f$ and a point $x$, is given by its gradient evaluated at $x$:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span><span class="p">(</span><span class="n">reduce_retracing</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">compute_grads_scratch</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">point</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">point</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">*</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">point</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">grad</span><span class="p">,</span> <span class="n">loss</span>
</code></pre></div></div>

<p>As our function describes loss or error, optimization is performed by navigating
the surface according to the negated gradient. I.e., the direction of highest descent:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">optimize_w_grads</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">wd</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">lr</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
  <span class="n">wd</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">wd</span><span class="p">)</span>
  <span class="n">point</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>

  <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">points</span> <span class="o">=</span> <span class="p">[</span><span class="n">point</span><span class="p">.</span><span class="n">numpy</span><span class="p">()]</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
      <span class="n">grad</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">compute_grads_scratch</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">point</span><span class="p">)</span>
      <span class="n">point</span><span class="p">.</span><span class="n">assign_add</span><span class="p">(</span><span class="o">-</span><span class="n">lr</span><span class="o">*</span><span class="n">grad</span> <span class="o">-</span><span class="n">lr</span><span class="o">*</span><span class="n">wd</span><span class="o">*</span><span class="n">point</span><span class="p">)</span>

      <span class="n">grads</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
      <span class="n">points</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">point</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
  <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error: </span><span class="si">{</span><span class="n">error</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

  <span class="n">grads</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
  <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">points</span><span class="p">,</span> <span class="n">grads</span>
</code></pre></div></div>

<h3 id="descriptive-optimization-functions">Descriptive Optimization Functions</h3>

<p>Now, all that’s left is to define a few optimization functions that will represent our problems:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">f0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
  <span class="c1"># Quadratic Equation Surface ($x^2+y^2$)
</span>  <span class="k">return</span> <span class="p">((</span><span class="n">x</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span><span class="mf">0.04</span><span class="o">*</span><span class="n">r</span> <span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">c</span>

<span class="k">def</span> <span class="nf">f1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
  <span class="c1"># Cubic Equation Surface ($x^3+y^3$)
</span>  <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">y</span> <span class="o">+</span> <span class="mf">0.06</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span><span class="mf">0.1</span><span class="o">*</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="o">/</span><span class="mi">400</span> <span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">r</span> <span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">c</span>

<span class="k">def</span> <span class="nf">f2</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
  <span class="c1"># Quadric Equation Surface ($x^2-y^2$)
</span>  <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="o">-</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mf">40.</span> <span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">r</span> <span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">c</span>

<span class="k">def</span> <span class="nf">f3</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
  <span class="c1"># Quadric Equation Surface ($x^2-y^2 -0.75xy$)
</span>  <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="o">-</span><span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span><span class="mf">1.5</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">y</span><span class="p">))</span><span class="o">/</span><span class="mi">60</span> <span class="o">+</span><span class="mf">0.05</span><span class="o">*</span><span class="n">r</span> <span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">c</span>

<span class="k">def</span> <span class="nf">f4</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
  <span class="c1"># Ramp Equation Surface ($\sin(x/5)-\cos(y/10)$)
</span>  <span class="k">return</span> <span class="p">((</span><span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mf">5.</span><span class="p">)</span> <span class="o">-</span> <span class="n">tf</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">y</span><span class="o">/</span><span class="mf">10.</span><span class="p">))</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span><span class="mf">0.05</span><span class="o">*</span><span class="n">r</span> <span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">c</span>

<span class="k">def</span> <span class="nf">f5</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>
  <span class="c1"># Elliptic Quadratic Surface ($x/a^2+x/b^2$)
</span>  <span class="k">return</span> <span class="p">((</span><span class="n">x</span><span class="o">/</span><span class="mi">8</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span><span class="o">/</span><span class="mi">12</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span><span class="mf">0.04</span><span class="o">*</span><span class="n">r</span> <span class="o">+</span><span class="mf">0.01</span><span class="o">*</span><span class="n">c</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseCtr1" aria-controls="collapseCtr1" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show plotting code</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseCtr1"><div class="highlight"><pre class="highlight"><code><span class="n">as_opt_set</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">tag</span><span class="o">=</span><span class="s">"SGD"</span><span class="p">:</span> <span class="p">({</span><span class="n">tag</span><span class="p">:</span> <span class="n">e</span><span class="p">}</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># f0
</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">optimize_w_grads</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f0</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="n">f0</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="o">*</span><span class="n">as_opt_set</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span>
  <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">animated</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"sgd-s0-quadratic.png"</span><span class="p">)</span>

<span class="c1"># f1
</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">optimize_w_grads</span><span class="p">(</span><span class="n">f1</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f1</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="n">f1</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="o">*</span><span class="n">as_opt_set</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span>
  <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">animated</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"sgd-s1-cubic.png"</span><span class="p">)</span>

<span class="c1"># f2
</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">optimize_w_grads</span><span class="p">(</span><span class="n">f2</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">7.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f2</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="n">f2</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="o">*</span><span class="n">as_opt_set</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">animated</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"sgd-s2-quadric.png"</span><span class="p">)</span>

<span class="c1"># f3
</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">optimize_w_grads</span><span class="p">(</span><span class="n">f3</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="o">-</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f3</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="n">f3</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="o">*</span><span class="n">as_opt_set</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">),</span> <span class="n">animated</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"sgd-s3-quadric.png"</span><span class="p">)</span>

<span class="c1"># f4
</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">optimize_w_grads</span><span class="p">(</span><span class="n">f4</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="o">-</span><span class="mf">7.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">wd</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f4</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="n">f4</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="o">*</span><span class="n">as_opt_set</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">),</span> <span class="n">animated</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">"sgd-s4-ramp.png"</span><span class="p">)</span>
</code></pre></div></div>

<div class="row w-xl-auto align-items-start">
  <div class="col-12 col-sm-6 col-md-4 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/surfaces/s0.png" alt="Quadratic Equation Surface:" class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Quadratic Equation Surface:</strong>

    $f(x, y) = x^2 +y^2$
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-sm-6 col-md-4 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/surfaces/s1.png" alt="Cubic Equation Surface:" class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Cubic Equation Surface:</strong>

    $f(x, y) = x^3 +y^3 +x^2 + y^2 + y$
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-sm-6 col-md-4 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/surfaces/s2.png" alt="Quadric Equation Surface:" class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Quadric Equation Surface:</strong>

    $f(x, y) = x^2-y^2$.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-sm-6 col-md-4 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/surfaces/s3.png" alt="Quadric Equation Surface:" class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Quadric Equation Surface:</strong>

    $f(x, y) = x^2-y^2 -xy$.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-sm-6 col-md-4 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/surfaces/s4.png" alt="Ramp Equation Surface:" class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Ramp Equation Surface:</strong>

    $f(x, y) = \sin(x)-\cos(y)$.
  </figcaption>


  </figure>
</div>

  </div>
  <!-- <div class="col-12 col-sm-6 col-md-4 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/surfaces/s5.png" alt="Ramp Equation Surface:" class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Ramp Equation Surface:</strong>

    $f(x, y) = x^3+y^3+x^2+y^2+x+y$.
  </figcaption>


  </figure>
</div>

  </div> -->
</div>

<h2 id="modern-optimization-methods">Modern Optimization Methods</h2>

<p>Modern optimization strategies are founded on the Gradient-based optimization idea, but
often rely on tricks to accelerate or stabilize the walking over the optimization surface.
In this section, I introduce some of the optimizers that have recently appeared in literature,
detailing their mathematical foundations,
and illustrating their optimizing rolling ball over the previously defined surfaces.</p>

<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>

<p>The simplest of the all is called Stochastic Gradient Descent (SGD). <a class="citation" href="#bottou2012stochastic">[3]</a>
In its most known form “mini-batch”, the core idea is to,at each training step $t$,
<strong>sample the training set</strong> for a batch $(\mathbf{X}_t, \mathbf{Y}_t) \subset (\mathbf{X}, \mathbf{Y})$
 sufficiently large to closely represent
the set, and yet sufficiently small so its gradient computation does not become prohibitive.</p>

<p>As batches may contain some noise, and gradients may not be good estimate for the
optimization direction of complex non-linear optimization surfaces,
It’s also standard procedure to adopt a learning rate, which can dampen the updates
applied to parameters, and, therefore, the mistakes made.</p>

<p>Stochastic Gradient Descent optimization is performed, at a training step $t$, as:</p>

\[\begin{align}\begin{split}
\bm g_t &amp;= \nabla_θ\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm θ_t &amp;= \bm θ_{t-1} - \eta \bm g_t
\end{split}\end{align}\]

<p>where $x$ is the sample batch, and $\bm θ_{t-1}$ is a vector, containing the model parameters, at step $t-1$.
We can visualize how SGD performs, for different learning rates, over a few of the previously defined surfaces:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_sgd_with_different_lrs</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"SGD lr: 0.1"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"SGD lr: 1.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"SGD lr:10.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">10.</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"SGD lr:20.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">20.</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"SGD lr:50.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">50.</span><span class="p">)),</span>
  <span class="p">]</span>

<span class="o">@</span><span class="n">tf</span><span class="p">.</span><span class="n">function</span><span class="p">(</span><span class="n">reduce_retracing</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">point</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">trainable_vars</span><span class="p">):</span>
  <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">point</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">point</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">r</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">point</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

  <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">trainable_vars</span><span class="p">)</span>
  <span class="n">opt</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">trainable_vars</span><span class="p">))</span>

  <span class="k">return</span> <span class="n">grads</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">loss</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseCtr3" aria-controls="collapseCtr3" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseCtr3"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">optimize_w_opt</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span> <span class="n">steps</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">point</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>
  <span class="n">trainable_vars</span><span class="p">,</span> <span class="n">points</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">point</span><span class="p">],</span> <span class="p">[</span><span class="n">point</span><span class="p">.</span><span class="n">numpy</span><span class="p">()],</span> <span class="p">[]</span>
  <span class="n">optimizer</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="n">trainable_vars</span><span class="p">)</span>

  <span class="k">try</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
      <span class="n">grad</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">point</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">trainable_vars</span><span class="p">)</span>
      <span class="n">grads</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
      <span class="n">points</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">point</span><span class="p">.</span><span class="n">numpy</span><span class="p">())</span>
  <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">error</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error: </span><span class="si">{</span><span class="n">error</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

  <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
          <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">optimize_all_w_opt</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]],</span> <span class="n">steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
  <span class="n">points_and_grads</span> <span class="o">=</span> <span class="p">[</span><span class="n">optimize_w_opt</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">)</span> <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">opt</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">]</span>
  <span class="n">opt_points</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">points</span> <span class="k">for</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">points_and_grads</span><span class="p">)}</span>
  <span class="n">opt_grads</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">grads</span> <span class="k">for</span> <span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">_</span><span class="p">),</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">opts</span><span class="p">,</span> <span class="n">points_and_grads</span><span class="p">)}</span>
  <span class="k">return</span> <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span>

<span class="n">opts</span> <span class="o">=</span> <span class="n">get_sgd_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">f0</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f0</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f0-sgd-lr01-50.gif"</span><span class="p">)</span>

<span class="n">opts</span> <span class="o">=</span> <span class="n">get_sgd_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f5</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">f5</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f5</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f5-sgd-lr01-50.gif"</span><span class="p">)</span>
</code></pre></div></div>

<div class="row w-lg-100 w-xl-175">
  <div class="col-12 col-md-6 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f0-sgd-lr01-50.gif" alt="SGD optimization over the Quadratic Function $f_0(x, y)=\frac{1}{2}[\frac{x}{10}^2 + \frac{y}{10}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">SGD optimization over the Quadratic Function $f_0(x, y)=\frac{1}{2}[\frac{x}{10}^2 + \frac{y}{10}^2]$.</strong>

    Most options converge to a similar point. Large lr values (e.g., 100) may diverge.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-md-6 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f5-sgd-lr01-50.gif" alt="SGD optimization over the Elliptic Quadratic Function $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">SGD optimization over the Elliptic Quadratic Function $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$.</strong>

    Optimization over elliptic surfaces converges slower than for perfectly circular surfaces.
  </figcaption>


  </figure>
</div>

  </div>
</div>

<p>Our goal is, of course, to reach the best fitted (lowest error) point as quickly as possible.
However, the only information available to us relates to a close neighborhood, described by the batch
and the current model’s parameters $θ_t$.
Gradient Descent must therefore navigate using the vector pointing to the direction of steepest descent (i.e., error decay).
This can be seen in the examples above: surface $f_0=x^2+y^2$ is a perfectly circular quadratic function,
while $f_5=\frac{x}{8}^2+\frac{y}{12}^2$ is an elliptic surface.
A variation in $x$ produces a much higher reduction in $f_5$ (i.e., $f_5(x-\delta, y) &lt; f_5(x, y-\delta)$, for $x\gg 0$),
the gradient is steered onto the direction of $x$.
Once it becomes sufficiently small, varying $x$ has a marginal overall impact on $f_5$’s value.
I.e., the contribution of $x$ to $f_5$ becomes small compared to $y$, reflecting a low derivative
value that marginally appears in the gradient.
Therefore, the gradient now steers towards the direction that minimizes the second term, $y$.</p>

<!-- NOTE -->
<div class="position-relative d-none d-xl-block">
<div class="card bg-light rounded-0 border-0 border-4 border-info font-small position-absolute start-100 ms-4 border-start" style="min-width:240px;">
<div class="card-body">
A list of Python implementations for linear decomposition methods can be found here:
<a href="https://scikit-learn.org/stable/api/sklearn.decomposition.html" target="_new">scikit-learn.org/decomposition</a>.
</div>
</div>
</div>

<p>The biased path taken by SGD has a detrimental effect on training: optimization is slower on elliptic surfaces,
where a curved path is taken, and overshooting is more likely to happen for smaller variables.
To mitigate this, many solutions employ decomposition methods that project data points onto a
representation space in which features are uncorrelated or unrelated.
Examples of such methods are: Singular Value Decomposition (SVD),
Principal Component Analysis (PCA), <a class="citation" href="#bro2014principal">[4]</a>
and Independent Component Analysis (ICA). <a class="citation" href="#lee1998independent">[5], [6]</a></p>

<h3 id="momentum">Momentum</h3>

<p>One way of bringing more stability to the training procedure is to average gradient vectors,
which can build inertia for the update vector, hence steering it onto a more stable
direction. <a class="citation" href="#thomas2021momentum">[7]</a>
To that end, Momentum is a simple extension of SGD that maintains the exponentially decaying
average of the update direction, using it to update weights instead:</p>

\[\begin{align}\begin{split}
\bm g_t &amp;= \nabla_θ\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm m_t &amp;= \beta\bm m_{t-1} -\eta\bm g_t  \\
\bm θ_t &amp;= \bm θ_{t-1} +\bm m_t
\end{split}\end{align}\]

<!-- NOTE -->
<div class="position-relative d-none d-xl-block">
<div class="card bg-light rounded-0 border-0 border-4 border-info font-small position-absolute end-100 me-4 border-start" style="min-width:240px;">
<div class="card-body">
A great in-depth analysis over Momentum's inner workings can be found in
Goh's article in Distill, "<a href="https://distill.pub/2017/momentum" target="_blank">Why Momentum Really Works</a>" <a class="citation" href="#goh2017whymomentumworks">[8]</a>.
</div>
</div>
</div>

<p>While highly effective, the accumulation of previous gradient information might
inadvertently move weights away from the local minimum point in abrupt changes
of the optimization surface’s curvature, occasionally leading to overshooting.
Hence, Momentum may benefit from the employment of a decaying learning rate policy.</p>

<h5 id="nesterov-accelerated-gradient">Nesterov Accelerated Gradient</h5>

<p>The Nesterov Accelerated Gradient (NAG), <a class="citation" href="#nesterov1983method">[9]</a> or Nesterov Momentum,
is a variation of Momentum design to alleviate overshooting occurrences.
It consists of calculating the gradient with respect to the updated data point
(i.e., the value we expect it will assume, considering its recent changes),
instead of its naive current value.</p>

<p>As $\beta\bm m_{t-1}$ contains the decaying average of the updates committed so far,
we expect the parameters to maintain a similar trend. Hence, we define an interim value for parameter
$\bm θ_t$:</p>

\[\begin{equation}
\hat{\bm θ}_t = \bm θ_{t-1} + \beta \bm m_{t-1} \approx \bm θ_t
\end{equation}\]

<p>And define NAG as:</p>

\[\begin{align}\begin{split}
\hat{\bm g_t} &amp;= \nabla_θ\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \hat{\bm θ}_t) \\
\bm m_t &amp;= \beta\bm m_{t-1} - \eta\hat{\bm g_t} \\
\bm θ_t &amp;= \bm θ_{t-1} + \bm m_t
\end{split}\end{align}\]

<p>It turns out that it’s not easy to compute $\nabla_θ\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \hat{\bm θ_t})$ without forwarding
the signal through the model twice. (Or maybe it could be done by patching the parameters’ values during the forward step?)
In any case, Nesterov is more commonly implemented <a class="citation" href="#dozat2016nadam">[10]</a> by approximating $\bm m_t$ instead:
<!-- assuming the gradient evaluated in $\bm θ_{t-1}$ is similar to the gradient at $\bm θ_t$, and --></p>

\[\begin{equation}
 \bm m_t \approx
 \hat{\bm m_t}
 = \beta\left[\beta\bm m_{t-1} - \eta\bm g_t\right]
 -\eta\bm g_t
\end{equation}\]

<p>Which brings us to NAG’s most common form:</p>

\[\begin{align}\begin{split}
\bm g_t &amp;= \nabla_θ\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm m_t &amp;= \beta\bm m_{t-1} - \eta\bm g_t \\
\bm θ_t &amp;= \bm θ_{t-1} + \beta \bm m_t - \eta\bm g_t
\end{split}\end{align}\]

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_momentum_with_different_lrs</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"NAG lr: 0.1"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"NAG lr: 1.0"</span><span class="p">,</span>  <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"NAG lr:10.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"NAG lr:20.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">20.0</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"NAG lr:50.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">50.0</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)),</span>
  <span class="p">]</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseCtr4" aria-controls="collapseCtr4" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseCtr4"><div class="highlight"><pre class="highlight"><code><span class="n">opts</span> <span class="o">=</span> <span class="n">get_momentum_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">f0</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f0</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f0-Momentum-lr01-50.gif"</span><span class="p">)</span>

<span class="n">opts</span> <span class="o">=</span> <span class="n">get_momentum_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f5</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">f5</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f5</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f5-momentum-lr01-50.gif"</span><span class="p">)</span>
</code></pre></div></div>

<div class="row w-lg-100 w-xl-175">
  <div class="col-12 col-md-6 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f0-momentum-lr01-50.gif" alt="Nesterov Momentum optimization over the Quadratic Function $f_0(x, y)=\frac{1}{2}[\frac{x}{10}^2 + \frac{y}{10}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Nesterov Momentum optimization over the Quadratic Function $f_0(x, y)=\frac{1}{2}[\frac{x}{10}^2 + \frac{y}{10}^2]$.</strong>

    Optimization quickly converges to local minima. Overshooting occurs for large learning rates.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-md-6 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f5-momentum-lr01-50.gif" alt="Nesterov Momentum optimization over the Elliptic Quadratic Function $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Nesterov Momentum optimization over the Elliptic Quadratic Function $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$.</strong>

    Optimization over $f_5$ converges slower than over $f_0$, while still being faster than SGD.
  </figcaption>


  </figure>
</div>

  </div>
</div>

<p>Momentum converges to close to the global minimum much faster than SGD for both
circular and elliptic surfaces, though the later is still slower than the former.
All learning rates lead to convergence, although overshooting
is more prominent for very large ones.
Small values still move at a reasonable pace (through velocity accumulation),
making the optimization process more robust to “unreasonable” choices for this hyperparameter.</p>

<h3 id="rmsprob">RMSProb</h3>

<p>NAG represents an improvement over SGD in convergence speed, but the difference
in weights norms is still noticeable, biasing the gradient towards larger weights,
and increasing the risk of overshooting.
Going in a different direction, Hinton et al. <a class="citation" href="#hinton2012rmsprop">[11]</a>
propose to divide the gradient of each weight by the moving average of its magnitude.</p>

<p>Named RMSprop, the idea behind this method is to normalize changes for each weight,
removing the biased gradient directions observed when optimizing with SGD or NAG, and making
it invariant to its magnitude throughout training.</p>

\[\begin{align}\begin{split}
\bm g_t &amp;= \nabla\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm v_t &amp;= \rho\bm v_{t-1} + (1-\rho)\bm g_t^2  \\
\bm θ_t &amp;= \bm θ_{t-1} -\eta \frac{\bm g_t}{\sqrt{\bm v_t}}
\end{split}\end{align}\]

<p>For $\rho=0$, the update rule becomes $\bm θ_{t-1} -\eta \frac{\bm g_t}{|\bm g_t|}$.
I.e., the sign of the gradient times the learning rate.
For larger values, the moving average will roughly point towards a more stable optimization direction,
reducing chances of abrupt changes in direction.
Furthermore, like gradient clipping, reducing the magnitude of the updates with RMSProp can inadvertently
reduce convergence time by damping the effect of noisy batches over updates.</p>

<p>Small variations of the RMSProp algorithm can exist in modern ML libraries. For example, one may consider
(a) combining RMSProp with Momentum by defining velocity vectors as decaying average of the amortized gradients (see Adam);
or (b) a “centered” version of RMSProp in which the gradient mean is subtracted from moving average of magnitudes,
and the gradient is normalized by an estimate of its variance.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_rmsprop_with_different_lrs</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"RMS lr:0.100"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"RMS lr:0.025"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.025</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"RMS lr:0.05"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"RMS lr:0.075"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.075</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"RMS lr:1.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)),</span>
  <span class="p">]</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseCtr5" aria-controls="collapseCtr5" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseCtr5"><div class="highlight"><pre class="highlight"><code><span class="n">opts</span> <span class="o">=</span> <span class="n">get_rmsprop_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">f0</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f0</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f0-Momentum-lr01-50.gif"</span><span class="p">)</span>

<span class="n">opts</span> <span class="o">=</span> <span class="n">get_rmsprop_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f5</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">f5</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f5</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">approximate_positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f5-momentum-lr01-50.gif"</span><span class="p">)</span>
</code></pre></div></div>

<div class="row w-lg-100 w-xl-175">
  <div class="col-12 col-md-6 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f0-rmsprop-lr01-50.gif" alt="RMSprop optimization over the Quadratic Function $f_0(x, y)=\frac{1}{2}[\frac{x}{10}^2 + \frac{y}{10}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">RMSprop optimization over the Quadratic Function $f_0(x, y)=\frac{1}{2}[\frac{x}{10}^2 + \frac{y}{10}^2]$.</strong>

    Optimization converges at a regular pace, and overshooting is seldom observed.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-md-6 col-lg">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f5-rmsprop-lr01-50.gif" alt="RMSprop optimization over the Elliptic Quadratic Function $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">RMSprop optimization over the Elliptic Quadratic Function $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$.</strong>

    Optimization over $f_5$ converges at a similar pace and direction of $f_0$, and at a constant rate.
  </figcaption>


  </figure>
</div>

  </div>
</div>

<p>With SGD and NAG, the step size is not only dictated by the learning rate $\eta$, but also by the
norm of the gradient. This implies a bigger updates over points where the optimization surface is
steeper, and smaller steps close to local minima. With RMSProp, a constant gradient norm (given a
constant learning rate) is observed throughout training, implying a <strong>constant update rate</strong>.
It’s also noticeable that RMSprop moves onto a similar path for both surfaces $f_0$ and $f_5$,
mitigating the biased gradient previously observed over elliptic surfaces.</p>

<!-- NOTE -->
<div class="position-relative d-none d-xl-block">
<div class="card bg-light rounded-0 border-0 border-4 border-info font-small position-absolute start-100 ms-4 border-start" style="min-width:240px;">
<div class="card-body">
The originally proposed RMSprop describes a multiplicative scheduling policy for the learning rate,
in which it would increase by a factor (e.g., 20%) if the sign of the last two gradients were the
same, and decrease otherwise.
</div>
</div>
</div>

<p>This also implies that progressively reducing the learning rate is paramount for the convergence
of the optimization process. This is illustrated in Fig. 11 and Fig. 12, where RMSProp (with learning rate
$1.0$) twitches around the $0$ point in later training stages.</p>

<h3 id="adagrad">AdaGrad</h3>

<p>AdaGrad was originally proposed by Duchi et al. in the paper
“<a href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" target="_blank">Adaptive sub-gradient methods for online learning and stochastic optimization</a>”. <a class="citation" href="#duchi2011adaptive">[12]</a>
The authors give a great introduction over it:</p>

<blockquote>
  <p>We present a new family of sub-gradient methods that dynamically incorporate knowledge of the
geometry of the data observed in earlier iterations to perform more informative gradient-based
learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of
very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic
 optimization and online learning which employ proximal functions to control the gradient steps
 of the algorithm. — Duchi et al. <a class="citation" href="#duchi2011adaptive">[12]</a></p>
</blockquote>

<p>The “geometry of the data” is represented by the mutual change in the components of the Gradient over the optimization surface.
Formally, AdaGrad updates weights according to the following rule.</p>

\[\begin{align}\begin{split}
\bm g_t &amp;= \nabla\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm G_t &amp;= \sum_\tau^t \bm g_\tau \bm g_\tau^\intercal \\
\bm v_t &amp;= \bm G_t^{-1/2} \bm g_t = \left[\sum_k^m \bm G_{t,\tau,k}^{-1/2} \bm g_{t,k} \right]_{0 \leq \tau &lt; m} \\
\bm θ_t &amp;= \bm θ_{t-1} -\eta \bm v_t
\end{split}\end{align}\]

<p>where $\bm G_t$ contains the sum of the outer products of all previous gradient vectors with themselves,
and $\bm G_t^{-1/2}$ is the inverse of the square root of matrix $\bm G_t$.</p>

<p>Multiplying $\bm G_t^{-1/2}$ by $\bm g_{t}$ projects the gradient onto a normalized vector space where the difference between its components is smaller, as the
change rate $\bm g_{t,i}$ of each parameter $i$ is scaled by the inverse of the sum of its absolute mutual change with every other parameter.
Parameters with strong variation will have their update rate dampened, approximating them to the change rate of the “comparatively rare features.” <a class="citation" href="#duchi2011adaptive">[12]</a></p>

<p>However, the time complexity required to compute the root square and the matrix inversion $\bm G_{t}^{-1/2}$ is $O(n^2)$,
implying higher computational footprint and much longer training times than the previous optimizers.
A simpler and less computationally intensive alternative is thus implemented by most machine learning frameworks.</p>

<!-- NOTE -->
<div class="position-relative d-none d-xl-block">
<div class="card bg-light rounded-0 border-0 border-4 border-info font-small position-absolute end-100 me-4 border-start" style="min-width:240px;">
<div class="card-body">
  Both <a href="https://github.com/keras-team/keras/blob/master/keras/src/optimizers/adagrad.py" target="_blank" class="link-info">Keras</a> and
  <a href="https://pytorch.org/docs/stable/_modules/torch/optim/adagrad.html#AdaGrad" class="link-info" target="_blank">Torch</a>
  implement the "simplified" AdaGrad.
</div>
</div>
</div>

<p>This “simpler” alternative only uses the main diagonal of $\bm G_t$, invariably assuming
that the remaining elements of the matrix are close to zero. That is, that most of the variation
observed is concentrated in the components’ second moments ($\bm G_{t,i,i}=\bm g_{t,i}^2 \gg 0$),
and that these components become somewhat linearly independent over time ($\bm G_{t,i,j} \approx 0$, for $i\neq j$):</p>

\[\begin{align}\begin{split}
\bm g_t &amp;= \nabla\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm G_t &amp;= \sum_\tau^t \bm g_\tau \bm g_\tau^\intercal \\
\bm v_t &amp;= \text{diag}\left(\bm G_t\right)^{-1/2} \odot \bm g_t = \left[ \frac{\bm g_{t,\tau}}{\sqrt{\bm G_{t,\tau,\tau} +\epsilon}} \right]_{0 \leq \tau &lt; m} \\
\bm θ_t &amp;= \bm θ_{t-1} -\eta \bm v_t
\end{split}\end{align}\]

<p>where $\odot$ is the <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)" target="_blank">Hadamard product</a> (element-wise multiplication).</p>

<h6 id="observing-changes-made-by-adagrad">Observing Changes Made by AdaGrad</h6>

<p>We can visualize what AdaGrad is doing by sampling un-normalized “gradients” from a random distribution and computing their associated $\bm G_t^{-1/2}$:</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>

<span class="k">def</span> <span class="nf">adagrad_w_full_gt</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">gt</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="n">scales</span>
    <span class="n">Gt</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">outer</span><span class="p">(</span><span class="n">gt</span><span class="p">,</span> <span class="n">gt</span><span class="p">)</span>
    <span class="n">Gt_inv</span> <span class="o">=</span> <span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">linalg</span><span class="p">.</span><span class="n">sqrtm</span><span class="p">(</span><span class="n">Gt</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Gt</span><span class="p">))</span><span class="o">*</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">vt</span> <span class="o">=</span> <span class="p">(</span><span class="n">Gt_inv</span> <span class="o">@</span> <span class="n">gt</span><span class="p">[...,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]).</span><span class="n">ravel</span><span class="p">()</span>
    <span class="k">yield</span> <span class="n">gt</span><span class="p">,</span> <span class="n">vt</span>

<span class="k">def</span> <span class="nf">adagrad_w_diag_gt</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">),</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">gt</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="n">scales</span>
    <span class="n">Gt</span> <span class="o">+=</span> <span class="n">gt</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">vt</span> <span class="o">=</span> <span class="n">gt</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">Gt</span><span class="p">)</span><span class="o">+</span><span class="mf">1e-5</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">gt</span><span class="p">,</span> <span class="n">vt</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseAdagradScalingPlot" aria-controls="collapseAdagradScalingPlot" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseAdagradScalingPlot"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">F</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">SCALES</span> <span class="o">=</span> <span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="n">SEED_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">SEED_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">53</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_adagrads</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)):</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

  <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="n">gt</span><span class="p">,</span> <span class="n">vt</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grads</span><span class="p">):</span>
    <span class="n">xytext</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.02</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02</span><span class="p">]</span>
    <span class="n">a_u</span> <span class="o">=</span> <span class="n">N</span><span class="o">/</span><span class="mi">2</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="o">*</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="o">-</span><span class="n">a_u</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">a_u</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">gt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">vt</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">vt</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s">'teal'</span><span class="p">,</span> <span class="n">head_width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s">"$\|g_</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s">\| = </span><span class="si">{</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gt</span><span class="p">):.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">$"</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">gt</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="n">gt</span><span class="o">+</span><span class="n">xytext</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s">"$\|v_</span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s">\| = </span><span class="si">{</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">vt</span><span class="p">):.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">$"</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="n">vt</span><span class="p">,</span> <span class="n">xytext</span><span class="o">=</span><span class="n">vt</span><span class="o">+</span><span class="n">xytext</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'teal'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>

  <span class="n">plt</span><span class="p">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">plot_adagrads</span><span class="p">(</span><span class="n">adagrad_w_full_gt</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">F</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="n">SCALES</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">SEED_1</span><span class="p">),</span> <span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">plot_adagrads</span><span class="p">(</span><span class="n">adagrad_w_diag_gt</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">F</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="n">SCALES</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">SEED_2</span><span class="p">),</span> <span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
</code></pre></div></div>

<div class="row w-lg-100 w-xl-175 align-items-start">
  <div class="col-12 col-md-6">
    <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/adagrad_scaling.png" alt="Components $\bm v_t$ (teal) derived from Gradients (gray) by AdaGrad's full version." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Components $\bm v_t$ (teal) derived from Gradients (gray) by AdaGrad's full version.</strong>

    Component $y$ is dampened in favor of component $x$, resulting in normalized change directions ($\|\bm v_0\| = 1$). Subsequent iterations move $\|\bm v_t\|$ towards $0$.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-md-6">
    <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/adagrad_scaling_2.png" alt="Components $\bm v_t$ (teal) derived from Gradients (gray) by AdaGrad using $\text{diag}\left(\bm G_t\right)$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Components $\bm v_t$ (teal) derived from Gradients (gray) by AdaGrad using $\text{diag}\left(\bm G_t\right)$.</strong>

    Component $y$ is dampened and $x$ increased, resulting in the update direction $\bm v_0 = (1, 1)$.
  </figcaption>


  </figure>
</div>

  </div>
</div>

<p>From the Figures above, we observe that:</p>

<ul>
  <li>
    <p><strong>AdaGrad “full” version.</strong>
As seen in Fig. 13, $\bm g_0$ is scaled down to $\bm v_0$, resulting in normalized
change directions ($|\bm v_0| = 1$). Subsequent steps maintain a lower normed update
direction $\bm v_t$ by dampening the $y$ component in favor of the $x$ one.
As $t$ increases, we expect $\bm g_t$ (and $\bm v_t$) to become smaller due to
$\bm θ_t$ being closer to local minima and the accumulation of $\bm G_t$.
Eventually, $\bm g_t$ should approximate to $0$, implying in the convergence of
$\bm G_t$ and of the training procedure (check the training illustrated in the
optimization surface bellow).</p>
  </li>
  <li>
    <p><strong>AdaGrad “diagonal” version.</strong>
With AdaGrad’s alternative implementation, illustrated in Fig. 14, the first gradient $\bm v_0$
is scaled down to have its individual components equal to $1$. This process produces a biased
update vector, as its direction is slightly changed.
Hence, $||\bm v_0|| = \sqrt{2} \approx 1.4$.
Following iterations of AdaGrad approximate $||\bm v_t||$ to zero.
Like in the full version, the update direction is amortized over time.</p>
  </li>
</ul>

<div class="w-xl-auto ms-xl-n4 bg-light pb-2 mt-4 mb-4">
<div class="container-fluid">
<div class="row">
<div class="col-12 col-xl-6 offset-xl-3 mt-2">

        <h6 id="performance">Performance</h6>

        <p>We can also measure the difference in execution time of both versions.
To make it a little bit more realistic, we consider feature vectors in the
1024-dimensional space and make their norms follow a log-normal distribution.</p>

        <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">F</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">SCALES</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">lognormal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">F</span><span class="p">)</span>
<span class="n">SEED_1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">SEED_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">53</span><span class="p">)</span>

<span class="o">%</span><span class="n">timeit</span> <span class="nb">list</span><span class="p">(</span><span class="n">adagrad_w_full_gt</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">F</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="n">SCALES</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">SEED_1</span><span class="p">))</span>
<span class="o">%</span><span class="n">timeit</span> <span class="nb">list</span><span class="p">(</span><span class="n">adagrad_w_diag_gt</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">F</span><span class="p">,</span> <span class="n">scales</span><span class="o">=</span><span class="n">SCALES</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">SEED_2</span><span class="p">))</span>
</code></pre></div>        </div>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>27.4 s ± 1.86 s per loop (mean ± std. dev. of 7 runs, 1 loop each)
619 µs ± 162 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
</code></pre></div>        </div>

        <p><i class="bi bi-arrow-return-right ms-3"></i>
Clearly, the full version would not scale for large weight matrices.</p>

      </div>
</div>
</div>
</div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_adagrad_with_different_lrs</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"AdaG lr: 0.01"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"AdaG lr: 0.10"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"AdaG lr: 0.50"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"AdaG lr: 1.00"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"AdaG lr:10.00"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)),</span>
  <span class="p">]</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseCtrAdagrad" aria-controls="collapseCtrAdagrad" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseCtrAdagrad"><div class="highlight"><pre class="highlight"><code><span class="c1">#@title $f_0$ - AdaGrad
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_adagrad_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f0</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f0</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f0-adagrad-lr-0001-10.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_5$ - AdaGrad
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_adagrad_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f5</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f5</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f5</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f5-adam-lr0001-10.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_4$ - AdaGrad
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_adagrad_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f4</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="o">-</span><span class="mf">7.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f4</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f4</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f4-adam-lr0001-10.gif"</span><span class="p">)</span>
</code></pre></div></div>

<div class="row w-lg-100 w-xl-175">
  <div class="col-12 col-md-6">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f5-adagrad-lr001-10.gif" alt="AdaGrad over $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">AdaGrad over $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$.</strong>

    Similarly to RMSProp, AdaGrad follows a fairly unbiased path in elliptical surfaces, moving at a normalized rate during the first steps. However, the update steps becomes visibly smaller at later steps, and no twitching around the optimal point is observed.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-md-6">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f4-adagrad-lr001-10.gif" alt="AdaGrad over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">AdaGrad over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$.</strong>

    Noise is detrimental to AdaGrad, which gets stuck at local minima for many choices of learning rate.
  </figcaption>


  </figure>
</div>

  </div>
</div>

<p>While the constant accumulation of $\bm G_t$ assures convergence, it can inadvertently cause a problem:
as $\bm G_t$ can become relatively large at early steps, the gradients might be nullified, implying
early convergence and an underfit solution. This is seen in Fig. 16, where many choice of learning
rate cause AdaGrad to get stuck at local minima.</p>

<p>AdaDelta is a direct extension of AdaGrad that aims to overcome the aforementioned limitation by
setting a decay rate $\rho\in[0, 1]$, and maintaining the Exponential Moving Average (EMA) of the
past squared gradients and squared updates.
RMSProp is identical to AdaDelta if the squared updates were discarded. <a class="citation" href="#daniel2021adagrad">[13]</a></p>

<h3 id="adam">Adam</h3>

<p>Adam is one of the most popular optimization algorithms nowadays.
Its name stands for “adaptive moment estimation”, <a class="citation" href="#kingma2014adam">[14]</a> and consists of
keeping track the moving average for the first and second moments (just like Momentum and RMSprop,
respectively), and using them to regularize the direction of update of the weights.</p>

<p>Formally, Adam is defined as:</p>

\[\begin{align}\begin{split}
\bm g_t &amp;= \nabla\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm m_t &amp;= \beta_1\bm m_{t-1} + (1-\beta_1) \bm g_t \\
\bm v_t &amp;= \beta_2\bm v_{t-1} + (1-\beta_2) \bm g_t^2  \\
\hat{\bm m_t} &amp;= \bm m_t/(1-\beta_1^t) \\
\hat{\bm v_t} &amp;= \bm v_t/(1-\beta_2^t) \\
\bm θ_t &amp;= \bm θ_{t-1} -\eta \frac{\hat{\bm m_t}}{\sqrt{\hat{\bm v_t} + \epsilon}}
\end{split}\end{align}\]

<p>where $\beta_1$ and $\beta_2$ are hyperparameters similar to Momentum’s $\beta$ and RMSProp’s
$\rho$, respectively, and  $\beta^t$ represents the hyperparameters $\beta$ to the power $t$.
These frequently assume high values (such as 0.9 and 0.999), indicating its
preference for retaining a long Gradient footprint throughout training.</p>

<p>Adam’s most noticeable difference to previous methods are the correction terms
$1/(1-\beta^t_1)$ and $1/(1-\beta^t_2)$, which are employed to counter-weight the
initial bias of the terms $\bm m_t$ and  $\bm v_t$ towards $0$, specially when $\beta_1$ and $\beta_2$
terms are very large. <a class="citation" href="#kingma2014adam">[14]</a>
Notice that, for $t=1$ (first training step), $\hat{\bm m_t} = \bm g_t$ and $\hat{\bm v_t} = \bm g_t^2$.
For a large $t$ (once many steps are taken), $\beta^t_1, \beta^t_2 \to 0$, hence
$\hat{\bm m_t} \to \bm m_t$ and $\hat{\bm v_t} \to \bm v_t$.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_adam_with_different_lrs</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"Adam lr: 0.001"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Adam lr: 0.01"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Adam lr: 0.1"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Adam lr: 1.0"</span><span class="p">,</span>  <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Adam lr:10.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">10.0</span><span class="p">)),</span>
  <span class="p">]</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseCtr6" aria-controls="collapseCtr6" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseCtr6"><div class="highlight"><pre class="highlight"><code><span class="c1">#@title $f_5$ - Adam
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_adam_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f5</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f5</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f5</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f5-adam-lr0001-10.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_4$ - Adam
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_adam_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f4</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="o">-</span><span class="mf">7.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f4</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f4</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f4-adam-lr0001-10.gif"</span><span class="p">)</span>
</code></pre></div></div>

<div class="row w-lg-100 w-xl-175">
  <div class="col-12 col-md-6">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f5-adam-lr0001-10.gif" alt="Adam over $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Adam over $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$.</strong>

    Adam is invariant to data correlation, while converging much faster than RMSprop.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-md-6">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f4-adam-lr0001-10.gif" alt="Adam over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Adam over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$.</strong>

    Noise still affects Adam's optimization, but it is less prominent than with RMSprop.
  </figcaption>


  </figure>
</div>

  </div>
</div>

<p>Firstly, Adam converges much faster than RMSprop (due to gradient first moment accumulation),
while many choices for the learning rate (between $0.01$ and $10$) seem to work for the noisy
ramp ($f_4$) and elliptic ($f_5$) surfaces. Adam is considered fairly robust to changes of
hyperparameters. <a class="citation" href="#goodfellow2016deeplearning">[15]</a></p>

<p>Less twitching — compared with RMSProp — is also observed at later training steps, when
the optimizing surface becomes flatter and the accumulated $\bm m_t$ becomes small.
Conversely, we can also see the solution getting stuck in local minima points when
tiny learning rates are employed to transverse noisy optimization surfaces (e.g., $f_5$).</p>

<p>Adam also has many variations.
One of these is AdaMax, also defined by Kingma et al. <a class="citation" href="#kingma2014adam">[14]</a>,
in which $\bm v_t$ is defined as $\max(\beta_2 \bm v_{t-1}, |\bm g_t|)$ (i.e., the $L^\infty$
norm of the gradient).
Another variation is NAdam, <a class="citation" href="#dozat2016incorporating">[16]</a>, which can be loosely seen as
“RMSprop with Nesterov momentum”.
All of these formulations are commonly implemented in modern machine learning frameworks:
<a href="https://keras.io/api/optimizers/adam">tf/Adam</a>,
<a href="https://keras.io/api/optimizers/adamax">tf/AdaMax</a>,
<a href="https://keras.io/api/optimizers/nadam">tf/NAdam</a>,
<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adam.html">torch/Adam</a>,
<a href="https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html">torch/AdaMax</a>, and
<a href="https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html">torch/NAdam</a>.</p>

<h3 id="lion">Lion</h3>

<p>In 2023, Chen et al. <a class="citation" href="#chen2024symbolic">[17]</a> proposed an algorithm to perform <em>program search</em>.
In it, an “infinite and sparse program space” is searched after an optimization method that is both
effective and memory-efficient.
Starting from an initial program (AdamW), the search algorithm results is the Lion (Evolved Sign
Momentum) optimizer, one of the first optimization methods devised with the assist of machine learning.</p>

<p>Lion is defined as:</p>

\[\begin{align}\begin{split}
\bm g_t       &amp;= \nabla\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\hat{\bm m_t} &amp;= \beta_1\bm m_{t-1} + (1-\beta_1) \bm g_t \\
\bm m_t       &amp;= \beta_2\bm m_{t-1} + (1-\beta_2) \bm g_t \\
\bm θ_t   &amp;= \bm θ_{t-1} -\eta \text{ sign}(\hat{\bm m_t})
\end{split}\end{align}\]

<p>where $\beta_1=0.9$ and $\beta_2=0.99$ by default.</p>

<p>Lion is similar to Adam and RMSprop, in which the gradient is accumulated and normalized.
However, it is more memory-efficient than Adam, as it does not rely on the
second-order moment to normalize the magnitude of the update step.
It is also reasonable to assume that the learning rate should be smaller than Adam’s,
considering that the sign function should produce an update vector with higher norm.
Of course, like RMSprop, scheduling the reduction of learning rate is important to assure convergence
at later stages of training.</p>

<!-- > We analyze the properties and limitations of Lion. Users should be aware that the uniform update calculated
using the sign function usually yields a larger norm compared to those generated by SGD and adaptive
methods. Therefore, Lion requires a smaller learning rate lr, and a larger decoupled weight decay λ to
maintain the effective weight decay strength. --- Chen et al. (2023) -->

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_lion_with_different_lrs</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"Lion lr:0.01"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lion</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lion lr:0.1"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lion</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.10</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lion lr:0.25"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lion</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lion lr:0.5"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lion</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.50</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lion lr:1.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lion</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.00</span><span class="p">)),</span>
  <span class="p">]</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseCtr7" aria-controls="collapseCtr7" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseCtr7"><div class="highlight"><pre class="highlight"><code><span class="c1">#@title $f_5$ - Lion
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_lion_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f5</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f5</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f5</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f5-lion-lr05-1.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_4$ - Lion
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_lion_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f4</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="o">-</span><span class="mf">7.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f4</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f4</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f4-lion-lr05-1.gif"</span><span class="p">)</span>
</code></pre></div></div>

<div class="row w-lg-100 w-xl-175">
  <!-- <div class="col-12 col-md">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f5-lion-lr05-1.gif" alt="Lion over $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Lion over $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$.</strong>

    Lion is also invariant to data correlation, but much more sensible to the choice of learning rate.
  </figcaption>


  </figure>
</div>

  </div> -->
  <div class="col-12 col-md">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f4-lion-lr05-1.gif" alt="Lion over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Lion over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$.</strong>

    Strong noise still affects Lion, though it seems robust enough to achieve global minima for most choices of learning rate.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-md">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f1-lion-lr05-1.gif" alt="Lion over $f_1(x, y) = x^3 +y^3 +x^2 + y^2 + y$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Lion over $f_1(x, y) = x^3 +y^3 +x^2 + y^2 + y$.</strong>

    A sufficiently large learning rate is enough to escape local minima.
  </figcaption>


  </figure>
</div>

  </div>
</div>

<!-- NOTE -->
<div class="position-relative d-none d-xl-block">
<div class="card bg-light rounded-0 border-0 border-4 border-info font-small position-absolute start-100 ms-4 border-start" style="min-width:240px;">
<div class="card-body">
Lion is particularly sensible to noise in information, which can be strengthened
by the sign function. Hence, it seems to work better with larger
batch sizes, in which sample noise is mitigated. <a class="citation" href="#chen2024symbolic">[17]</a>
</div>
</div>
</div>

<p>Lion is much more sensible to a choice of learning rate, becoming highly unstable and resulting in
overshooting for large values. On the other hand, small values produce slower (but steady) progress towards
local minima, while still being robust against bumps in a noisy optimization surface.</p>

<p>Even though momentum can still be accumulated in steep regions of the optimization space,
Lion will converge slower than Adam due to the restrictive constraint applied to the update
step (the sign function).
We also observe twitching at later stages of training over surfaces with
easily found minimum points ($f_4$ and $f_5$), when employing a constant learning rate.</p>

<h3 id="lars--lamb">LARS &amp; LAMB</h3>

<p>While the previous methods are the most popular choices for daily tasks,
achieving fast convergence for many problems and reasonable batch sizes (higher than 8
and lower than 8,192),
one may wish to increase batch size even further, given compatible hardware is available.
<strong>Huge batch sizes</strong> allow us to present the entire dataset in the model’s training
loop in just a few steps, which could greatly <strong>reduce training time</strong>.</p>

<p>However, simply increasing the batch size tends to have a negative impact on accuracy,
as (a) <strong>fewer steps will be taken</strong>, each still limited by the learning rate;
and (b) a large quantity of samples (associated with many different tasks)
may result in strong coefficients from a great number of features,
<strong>adding strong noise to the gradient</strong>.
Therefore, naively increasing batch can result in a sub-optimal training.</p>

<p>A common practice is “linear LR scaling”: to increase learning rate proportionally to the
batch size, <a class="citation" href="#krizhevsky2014one">[18]</a> thus accounting for the increase in stability and reduction in number of steps,
but even that has a limit: using SGD, AlexNet and ResNet-50 cannot learn the ImageNet
dataset with batches larger than 1,024 and 8,192 samples, respectively.
In practice, increasing both learning rate and batch size above a number (such as 512)
is troublesome, as training becomes unstable due to noise injected by exaggerated gradients
devised from noisy batches and large learning rates.</p>

<h5 id="lars">LARS</h5>

<p>While a range of techniques were devised to improve upon this limit (such as scheduling
an increasing learning rate at early training stages), one quickly gained notoriety in the
Literature:
Layer-wise Adaptive Rate Scaling (LARS) <a class="citation" href="#you2017scaling">[19]</a> is an (per-weight)
adaptive strategy that comprises calculating the ratio between the $L_2$-norms of
the weight and its Gradient.</p>

<!-- NOTE -->
<div class="position-relative d-none d-xl-block">
<div class="card bg-light rounded-0 border-0 border-4 border-info font-small position-absolute start-100 ms-4 border-start" style="min-width:240px;">
<div class="card-body">
So far, we have represented weights by the linearization and concat of all layers' parameters.
However, LARS processes weights based on their respective layer.
To proceed, we need to aggregate them in here as well.
</div>
</div>
</div>

<blockquote>
  <p>If LR is large comparing to the ratio for some layer, then training may becomes unstable. The LR
“warm-up” attempts to overcome this difficulty by starting from small LR, which can be safely used
for all layers, and then slowly increasing it until weights will grow up enough to use larger LRs.
We would like to use different approach.
We use local LR $\bm λ_l$ for each layer $l$. — You et al. <a class="citation" href="#you2017scaling">[19]</a></p>
</blockquote>

<p>LARS redefines the update delta in SGD by considering a “local LR” $\bm λ^l$ for each layer $l$,
comprising parameters $\bm θ_t^l \subset \bm θ_t$:</p>

\[\begin{align}\begin{split}
\bm g_t &amp;= \nabla_θ\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm λ_t &amp;= \eta \left[\frac{\|\bm θ_{t-1}^l\|_2}{\|\bm g_t^l\|_2}\right]_{0 \le i &lt; m \mid i \in [θ_{t-1}^l]} \\
\bm θ_t &amp;= \bm θ_{t-1} -γ_t \bm λ_t \odot \bm g_t
\end{split}\end{align}\]

<p>where $γ_t$ is a scheduling factor for the learning rate, and $\eta$ is the LARS coefficient:
a “local” learning rate base value used to express our confidence in $\bm λ^l$.</p>

<p>Intuitively, LARS’s normalized update vector $\bm g_t / ||\bm g_t||_2$ becomes independent to the
magnitude of the Gradient, while still being compatible with the weight norm, hence
mitigating the exploding and vanishing gradient problems. The step size will increase
proportional to the parameter’s norm, as large parameters are expected to be associated with large
(and yet meaningful) Gradients.</p>

<p>The authors further consider the effect of weight decay on the update direction, as well as momentum,
when computing $\bm λ_t$:</p>

\[\begin{align}\begin{split}
\bm λ_t &amp;= \eta \left[\frac{\|\bm θ_{t-1}^l\|_2}{\|\bm g_t^l\|_2 + \beta \|\bm θ_{t-1}^l\|_2}\right]_{0 \le i &lt; m \mid i \in [θ_{t-1}^l]} \\
\bm m_t &amp;= \beta\bm m_{t-1} + γ_t \bm λ_t \odot (\bm g_t + \beta\bm θ_{t-1})  \\
\bm θ_t &amp;= \bm θ_{t-1} -\bm m_t
\end{split}\end{align}\]

<p>You et al. successfully trained models with batch sizes as large as 32k,
without displaying expressive loss in effectiveness,
using momentum $=0.9$, weight decay $=0.0005$, $γ_0 = 2.9$, and $\eta=0.0001$.</p>

<h5 id="lamb">LAMB</h5>

<p>LAMB is devised by the same authors behind LARS, and both algorithms are detailed in the paper
“Large batch optimization for deep learning: Training bert in 76 minutes”. <a class="citation" href="#you2019large">[20]</a>
This algorithm follows the same idea in LARS, but instead using Adam as base optimizer.</p>

<p>LAMB update step is defined as follows.</p>

\[\begin{align}\begin{split}
\bm g_t &amp;= \nabla\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm m_t &amp;= \beta_1\bm m_{t-1} + (1-\beta_1) \bm g_t \\
\bm v_t &amp;= \beta_2\bm v_{t-1} + (1-\beta_2) \bm g_t^2  \\
\hat{\bm m_t} &amp;= \bm m_t/(1-\beta_1^t) \\
\hat{\bm v_t} &amp;= \bm v_t/(1-\beta_2^t) \\
\bm r_t &amp;= \frac{\hat{\bm m_t}}{\sqrt{\hat{\bm v_t}}+\epsilon} \\
\bm λ_t &amp;= \eta \left[\frac{\phi(\|\bm θ_{t-1}^l\|_2)}{\|\bm r_t^l\|_2 + \beta \|\bm θ_{t-1}^l\|_2}\right]_{0 \le i &lt; m \mid i \in [θ_{t-1}^l]} \\
\bm θ_t &amp;= \bm θ_{t-1} -\eta \bm λ_t \odot (\bm r_t + \beta\bm θ_{t-1})  \\
\end{split}\end{align}\]

<p>where $\phi$ is a strictly positive dampening function that plays a similar role to the LARS coefficient,
constrained by $\alpha_l \le \phi(v) \le \alpha_u, \forall v &gt; 0 \mid \alpha_l, \alpha_u &gt; 0$.
In practice, LAMB seems to be implemented by most frameworks with $\phi(x) = x$ and a few safe
checks for zeros that could erase the update step.</p>

<div class="w-xl-auto ms-xl-n4 bg-light pb-2 mt-4 mb-4">
<div class="container-fluid">
<div class="row">
<div class="col-12 col-xl-6 offset-xl-3 mt-2">

        <h6 id="lipschitz-constant">Lipschitz Constant</h6>

        <p>There is an association between LARS and the Lipschitz constant. To refresh your memory, 
Lipschitz continuity refers to a property of functions that limits how fast they can change. All
continuously differentiable functions are also Lipschitz continuous. Formally, a real-valued
function $f: \mathbb{R}\to\mathbb{R}$ is Lipschitz continuous if there exists a positive real
constant $K$ such that</p>

\[|f(x_1)-f(x_2)| \le K|x_1 - x_2|, \forall x_1,x_2 \in\mathbb{R}^f\]

        <p>Particularly, it holds true for $x_1 = \lim_{h\to 0} x_2+h$. Hence,</p>

\[\begin{align}\begin{split}
&amp;\lim_{h\to 0} |f(x_2+h)-f(x_2)|                                \le K|x_2+h - x_2| \\
\implies &amp;\lim_{h\to 0} \frac{|f(x_2+h)-f(x_2)|}{|x_2+h - x_2|} \le K \\
\implies &amp; \left| \frac{\partial}{\partial x} f(x) \right| \le K
\end{split}\end{align}\]

        <p>A multi-variate function $f: \mathbb{R}^f\to\mathbb{R}$ is said to be $L_i$-smooth w.r.t. $\bm x_i$ if there exists
a constant $K_i$ such that</p>

\[\begin{equation}
||\nabla_i f(\bm x_1) - \nabla_i f(\bm x_2)||_2 \le K_i||x_{1,i} - x_{2,i}||_2, \forall \bm x_1, \bm x_2 \in \mathbb{R}^f
\end{equation}\]

        <!-- NOTE -->
        <div class="position-relative d-none d-xl-block">
<div class="card bg-custom-gray rounded-0 border-0 border-4 border-info font-small position-absolute start-100 ms-4 mt-2 border-start" style="min-width:240px;">
<div class="card-body">
For $\eta=1$ and $\phi(v)=v$, <br />
$\bm λ_t \approx 1/\bm K$.
</div>
</div>
</div>

        <p>where $\bm K = [K_1, K_2, \ldots, K_i, \ldots, K_f]^\intercal$ is the $f$-dimensional vector of Lipschitz constants.</p>

      </div>
</div>
</div>
</div>

<p>This definition is used in the convergence theorem for SGD.
To converge means to move towards a stationary point $x_T$ after $T$ steps, and thus
$\lim_{t\to T} ||\nabla f(x_t)||_2 = 0$. This is an important aspect in optimization,
describing that the training procedure will find a solution — though not necessarily the
global optimal one.</p>

<p>The following convergence theorem is adopted and mentioned by LARS’s authors:</p>

<hr class="mb-4" />

<!-- NOTE -->
<div class="position-relative d-none d-xl-block">
<div class="card bg-light rounded-0 border-0 border-4 border-info font-small position-absolute start-100 ms-4 border-start" style="min-width:240px;">
<div class="card-body">
      <p>Other proofs include the Classical Convergence Theorem, introduced in this
<a href="https://www.youtube.com/watch?v=a4xaK0dsZ_c">video</a>, or stated more rigorously
in this <a href="https://arxiv.org/abs/2301.11235" target="_blank">handbook</a> <a class="citation" href="#garrigos2023handbook">[21]</a>.</p>
    </div>
</div>
</div>

<p><strong>Theorem</strong> (<a href="https://arxiv.org/pdf/1309.5549">Ghadimi &amp; Lan, 2013</a> <a class="citation" href="#ghadimi2013stochastic">[22]</a>).
With large batch $b = T$ and using appropriate learning rate,
we have the following for the iterates of SGD:</p>

\[\begin{equation}
\mathbb{E}\left[||\nabla f(x_a)||_2^2\right] \le \mathcal{O}\left(\frac{(f(x_1) − f(x^\star)) L_\infty}{T} + \frac{||\sigma||^2_2}{T}\right)
\end{equation}\]

<p>where $x_\star$ is an optimal solution to the problem, and $x_a$ is an iterate uniformly randomly
chosen from ${x_1, \ldots, x_T}$.</p>
<hr />

<p>That means that the square norm of Gradient is in the order of the ratio between (i) the distance
$f(x_a) - f(x_\star)$ time the maximum Lipschitz constant $L_\infty = \sup_i |\bm L|$, and
(ii) the number of steps $T$, plus a tolerated noise factor $||\sigma||^2_2 / T$.</p>

<p>Conversely, You et al. devise the following convergence theorem for LARS:</p>

<hr class="mb-4" />

<p><strong>Theorem</strong> (<a href="https://arxiv.org/pdf/1904.00962">You et al., 2020</a> <a class="citation" href="#you2019large">[20]</a>).
Let $\eta_t = \eta = \sqrt{\frac{2(f(x_1)−f(x_\star))}{α_u^2||L||_1 T}} $ for all $t ∈ [T]$, $b = T$, $α_l ≤ φ(v) ≤ α_u$ for all $v &gt; 0$
where $α_l, α_u &gt; 0$. Then for $x_t$ generated using LARS, we have the following bound:</p>

\[\begin{equation}
\left(\mathbb{E}\left[\frac{1}{\sqrt{h}} \sum_i^h || \nabla_i f(x_a)||_2 \right]\right)^2
  ≤ \mathcal{O}\left(\frac{(f(x_1) − f(x_\star))L_\text{avg}}{T} + \frac{||\sigma||_1^2}{Th}\right)
\end{equation}\]

<p>where $x_\star$ is an optimal solution to the problem and $x_a$ is an iterate uniformly randomly
chosen from ${x_1, \ldots, x_T}$.</p>

<hr />

<p>The authors remark that being bound by $L_\text{avg}$ is an important aspect of LARS,
as it tends to be considerably smaller than $L_\infty$ (the highest Lipschitz constant).</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_lamb_with_different_lrs</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"Lamb lr:0.001"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lamb</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lamb lr:0.01"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lamb</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lamb lr:0.1"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lamb</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lamb lr:0.2"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lamb</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lamb lr:0.5"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lamb</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)),</span>
  <span class="p">]</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseCtr7" aria-controls="collapseCtr7" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseCtr7"><div class="highlight"><pre class="highlight"><code><span class="c1">#@title $f_5$ - LAMB
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_lamb_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f5</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f5</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f5</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f5-lamb-lr001-05.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_4$ - LAMB
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_lamb_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f4</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f4</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f4</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f4-lamb-lr001-05.gif"</span><span class="p">)</span>
</code></pre></div></div>

<div class="row w-lg-100 w-xl-175">
  <div class="col-12 col-md">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f5-lamb-lr001-05.gif" alt="LAMB over $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">LAMB over $f_5(x, y)=\frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$.</strong>

    LAMB converges fast for almost all choices of learning rate, and escapes local minima with ease.
  </figcaption>


  </figure>
</div>

  </div>
  <div class="col-12 col-md">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f4-lamb-lr001-05.gif" alt="LAMB over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">LAMB over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$.</strong>

    A strong momentum effect is noticeable, causing it to quickly overshoot the local minima.
  </figcaption>


  </figure>
</div>

  </div>
  <!-- <div class="col-12 col-md">
  <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f1-lamb-lr001-05.gif" alt="LAMB over $f_1(x, y) = x^3 +y^3 +x^2 + y^2 + y$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">LAMB over $f_1(x, y) = x^3 +y^3 +x^2 + y^2 + y$.</strong>

    A sufficiently large learning rate is enough to escape local minima.
  </figcaption>


  </figure>
</div>

  </div> -->
</div>

<p>LAMB converges much faster than Adam.
Notice that I dampened the learning rates (increasing lower ones and decreasing large ones)
so the updates could still be visible in the animation.</p>

<p>The overshooting seen in Momentum-based methods is even more prominent here, as the updates are
further scaled up by $\bm λ_t$ during initial steps when the parameters norms are usually high —
though, granted, this phenomenon might be somewhat exaggerated here: it’s unlikely that real model
parameters will assume absolute values as large as 10.</p>

<p>Naturally, LAMB decelerates when $\bm θ_t \to 0$, the point where $\bm λ_t$’s numerator is
at its lowest.
A decaying learning rate scheduling policy plays an important role in convergence for this method.</p>

<h3 id="ftrl">FTRL</h3>

<p>FTRL is an optimization strategy that aims to achieve a good (effective) and sparse (regularized) solution.
Its acronym stands for “Follow The (Proximally) Regularized Leader”, and it is first described by McMahan et al. <a class="citation" href="#mcmahan2013ad">[23]</a></p>

<p>FTRL’s update step is performed as follows.</p>

\[\begin{align}\begin{split}
\bm g_{t} &amp;= \nabla_θ\mathcal{L}(\mathbf{X}_t, \mathbf{Y}_t, \bm θ_{t-1}) \\
\bm g_{1:t} &amp;= \sum_{s=1}^t \bm g_s \\
\bm θ_{t} &amp;= \arg\min_{\bm θ} \left(\bm g_{1:t} \cdot \bm θ +\frac{1}{2}\sum_s^t \sigma_s \|\bm θ - \bm θ_s\|_2^2 +λ_1\|\bm θ\|_1\right)
\end{split}\end{align}\]

<p>where $\eta_t$ is the learning rate schedule originally set in the paper, inversely proportional to the training step ($\eta_t=\frac{1}{\sqrt{t}}$) and $\sigma_t$ is the $L_2$ coefficient s.t. $\sigma_{1:t}=\frac{1}{\eta_t}$.
A hasten read of Eq. 12 may suggest to the reader the necessity of keeping track of all previous $\bm θ_s$.
Fortunately, this equation can be rewritten <a class="citation" href="#mcmahan2013ad">[23]</a> as:</p>

\[\bm θ_{t} = \arg\min_{\bm θ} \left(\bm g_{1:t} - \sum_s^t \sigma\bm θ_{t}\right) \cdot \bm θ +\frac{1}{\eta_t} \|\bm θ\|_2^2 +λ_1\|\bm θ\|_1 +\text{(const)}\]

<p>FTRL employs both $L_1$ and $L_2$ regularization directly into the optimization objective function,
modulated by hyperparameters $λ_1$ and $\sigma_s$, respectively; and it has been successfully
employing when training large Natural Language Processing models, often trained
over tasks represented by high-dimensional, sparsely represented feature spaces.</p>

<p>FTRL is implemented as follows in tensorflow/Keras:
<!-- Let $n=0$, $\sigma_0=0$, and $z=0$, and $\gamma = 0.5$. --></p>

\[\begin{align}\begin{split}
\bm v_t &amp;= \bm v_{t-1} + \bm g_t^2 \\
\bm\sigma_t &amp;= \frac{1}{\eta_0}(\bm v_t^{\gamma} - \bm v_{t-1}^{\gamma}) \\
\bm z_t &amp;= \bm z_{t-1} + \bm g_t - \bm\sigma_t \bm θ_{t-1} \\
\bm θ_{t} &amp;= \begin{cases}
  \frac{\text{sign}(\bm z_t) λ_1 - \bm z_t}{(\beta + \sqrt{\bm v_t + \epsilon}) / \alpha + λ_2} \text{, if } |\bm z_t| &gt;= λ_1 \\
  0 \text{, otherwise}
\end{cases}
\end{split}\end{align}\]

<p>where $\gamma$ is the learning rate reduction rate, commonly defined as $-0.5$.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_ftrl_with_different_lrs</span><span class="p">():</span>
  <span class="k">return</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"FTRL lr: 0.001"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Ftrl</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"FTRL lr: 0.01"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Ftrl</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.010</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"FTRL lr: 0.1"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Ftrl</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.100</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"FTRL lr: 1.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Ftrl</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.000</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"FTRL lr:10.0"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Ftrl</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">10.000</span><span class="p">)),</span>
  <span class="p">]</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapseCtr8" aria-controls="collapseCtr8" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapseCtr8"><div class="highlight"><pre class="highlight"><code><span class="c1">#@title $f_5$ - FTRL
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_ftrl_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f5</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f5</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f5</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">40</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f5-ftrl-lr001-10.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_4$ - FTRL
</span><span class="n">opts</span> <span class="o">=</span> <span class="n">get_ftrl_with_different_lrs</span><span class="p">()</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">f4</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="p">[[</span><span class="o">-</span><span class="mf">7.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">f4</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">f4</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>
     <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span>
  <span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"f4-ftrl-lr001-10.gif"</span><span class="p">)</span>
</code></pre></div></div>

<div class="row w-lg-100 w-xl-175">
  <div class="col-12 col-md">
    <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f4-ftrl-lr001-10.gif" alt="FTRL over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">FTRL over $f_4(x, y)=\sin(\frac{x}{5})-\cos(\frac{y}{10})$.</strong>

    Large learning rates result in fast convergence towards global minimum. Conversely, a small learning rate result in the algorithm being stuck at local noisy regions.
  </figcaption>


  </figure>
</div>

    </div>
  <div class="col-12 col-md">
    <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/optimizers/f1-ftrl-lr001-10.gif" alt="FTRL over $f_1(x, y) = x^3 +y^3 +x^2 + y^2 + y$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">FTRL over $f_1(x, y) = x^3 +y^3 +x^2 + y^2 + y$.</strong>

    FTRL quickly converges to $(0, 0)$, and even large learning rates are not enough to escape from local minimum.
  </figcaption>


  </figure>
</div>

  </div>
</div>

<p>In my experiments, FTRL converged much faster than other algorithms, though it was particularly sensitive to noise, plateaus, and local minima.
More specifically, most configurations resulted in a less-than-optimal solution for surface $f_4$,
and no configuration was sufficient to transverse the local optimum plateau found in $f_1$
(which is easily optimized by Momentum, Adam and Lion).</p>

<h2 id="surface-transversing-with-modern-optimizers">Surface Transversing with Modern Optimizers</h2>

<p>With a few tweaks, we can visualize how each and every of these optimization methods perform over
the surface equations previously defined.
Some of them are adaptive, and some are not. Thus, we are forced to use different hyper-parameters for each one of them.
I picked the ones that shown good results in the previous sections, but doing some cross-validation over each surface
might result in better behavior from some (or all) of the algorithms.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_optimizers</span><span class="p">(</span><span class="n">wd</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"SGD"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"NAG"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"RMS"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"AdaGrad"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adagrad</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">10.</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Adam"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span> <span class="ow">or</span> <span class="mf">0.</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Adamax"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adamax</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"NAdam"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Nadam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lion"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lion</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"Lamb"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Lamb</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)),</span>
    <span class="p">(</span><span class="s">"FTRL"</span><span class="p">,</span> <span class="n">tf</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Ftrl</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">wd</span><span class="p">)),</span>
  <span class="p">]</span>
</code></pre></div></div>

<div class="text-center mb-1" style="height:1.25rem">
  <hr class="thin mb-0 border-1 sr-none " />
  <button data-bs-target="#collapsePlotOpts" aria-controls="collapsePlotOpts" class="btn rounded-5 btn-light" style="position: relative; margin-top: -2rem;" type="button" data-bs-toggle="collapse" aria-expanded="false">show collapsed</button>
</div>

<div class="language-py collapse highlighter-rouge" id="collapsePlotOpts"><div class="highlight"><pre class="highlight"><code><span class="c1">#@title $f_0$
</span><span class="n">_f</span> <span class="o">=</span> <span class="n">f0</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]]</span>
<span class="n">opts</span> <span class="o">=</span> <span class="n">get_optimizers</span><span class="p">(</span><span class="n">wd</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">_f</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">_f</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">_f</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span><span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"suf_00.gif"</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_loss_in_time</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">zlim</span><span class="o">=</span><span class="s">"auto"</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"loss_00.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_1$
</span><span class="n">_f</span> <span class="o">=</span> <span class="n">f1</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">9.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]]</span>
<span class="n">opts</span> <span class="o">=</span> <span class="n">get_optimizers</span><span class="p">(</span><span class="n">wd</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">_f</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">_f</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">_f</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span><span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(.</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"suf_01.gif"</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_loss_in_time</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">zlim</span><span class="o">=</span><span class="s">"auto"</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"loss_01.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_2$
</span><span class="n">_f</span> <span class="o">=</span> <span class="n">f2</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">7.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">]]</span>
<span class="n">opts</span> <span class="o">=</span> <span class="n">get_optimizers</span><span class="p">(</span><span class="n">wd</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">_f</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">_f</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">_f</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span><span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"suf_02.gif"</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_loss_in_time</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">zlim</span><span class="o">=</span><span class="s">"auto"</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"loss_02.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_3$
</span><span class="n">_f</span> <span class="o">=</span> <span class="n">f3</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mf">9.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.5</span><span class="p">]]</span>
<span class="n">opts</span> <span class="o">=</span> <span class="n">get_optimizers</span><span class="p">(</span><span class="n">wd</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">_f</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">_f</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">_f</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span><span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">1.4</span><span class="p">,</span> <span class="mf">1.4</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"suf_03.gif"</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_loss_in_time</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">zlim</span><span class="o">=</span><span class="s">"auto"</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"loss_03.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_4$
</span><span class="n">_f</span> <span class="o">=</span> <span class="n">f4</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mf">7.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]]</span>
<span class="n">opts</span> <span class="o">=</span> <span class="n">get_optimizers</span><span class="p">(</span><span class="n">wd</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">_f</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">_f</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">_f</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span><span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">70</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"suf_04.gif"</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_loss_in_time</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">zlim</span><span class="o">=</span><span class="s">"auto"</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"loss_04.gif"</span><span class="p">)</span>

<span class="c1">#@title $f_5$
</span><span class="n">_f</span> <span class="o">=</span> <span class="n">f5</span>
<span class="n">p0</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">9.0</span><span class="p">,</span> <span class="mf">9.0</span><span class="p">]]</span>
<span class="n">opts</span> <span class="o">=</span> <span class="n">get_optimizers</span><span class="p">(</span><span class="n">wd</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span> <span class="o">=</span> <span class="n">optimize_all_w_opt</span><span class="p">(</span><span class="n">_f</span><span class="p">,</span> <span class="n">opts</span><span class="p">,</span> <span class="n">p0</span><span class="o">=</span><span class="n">p0</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">_f</span><span class="p">(</span><span class="n">z</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">z</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">z</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span>  <span class="c1"># Loss at every grid-point.
</span><span class="n">y</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span> <span class="n">_f</span><span class="p">(</span><span class="n">points</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">*</span><span class="n">positional_noise</span><span class="p">(</span><span class="n">points</span><span class="p">)).</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">points</span> <span class="ow">in</span> <span class="n">opt_points</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">handlers</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_surface_and_steps</span><span class="p">(</span><span class="n">opt_points</span><span class="p">,</span> <span class="n">opt_grads</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">100</span><span class="p">),</span> <span class="n">zlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.3</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"suf_05.gif"</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">ani</span> <span class="o">=</span> <span class="n">plot_loss_in_time</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">zlim</span><span class="o">=</span><span class="s">"auto"</span><span class="p">)</span>
<span class="n">ani</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"loss_05.gif"</span><span class="p">)</span>
</code></pre></div></div>

<div id="carouselSurfaceWOpts" class="carousel carousel-dark slide w-100 w-xl-auto mb-2">
  <div class="carousel-inner">
    <div class="carousel-item active">
      <div class="row d-flex align-items-end">
        <div class="col-12 col-md-4 offset-md-2">
        <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/suf_00.gif" alt="Optimizers moving on the quadratic surface $f_0(x, y) = x^2 +y^2$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Optimizers moving on the quadratic surface $f_0(x, y) = x^2 +y^2$.</strong>

    FTRL goes straight to the minimum with few steps. NAG also moves fast towards the minimum point, but overshoots once.
  </figcaption>


  </figure>
</div>

        </div>
        <div class="col-12 col-md-4">
        <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/loss_00.gif" alt="Loss over time for $f_0$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Loss over time for $f_0$.</strong>

    RMS presents a more stable convergence, followed by SGD.
  </figcaption>


  </figure>
</div>

        </div>
      </div>
    </div>
    <div class="carousel-item">
      <div class="row d-flex align-items-end">
        <div class="col-12 col-md-4 offset-md-2">
        <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/suf_05.gif" alt="Optimizers moving on $f_5 = \frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Optimizers moving on $f_5 = \frac{1}{2}[\frac{x}{8}^2 + \frac{y}{12}^2]$.</strong>

    NAG, Adam and Lion overshoot. Lion does so farther, but corrects itself faster as well. SGD is the slowest. Once again, FTRL goes almost instantly to the best solution.
  </figcaption>


  </figure>
</div>

        </div>
        <div class="col-12 col-md-4">
        <div style="max-width:540px" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/loss_05.gif" alt="Loss over time for the elliptic surface $f_5(x, y)$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Loss over time for the elliptic surface $f_5(x, y)$.</strong>

    Lion oscillates near the optimal solution and doesn't converge: a scheduled reduce to learning rate should fix this.
  </figcaption>


  </figure>
</div>

        </div>
      </div>
    </div>
    <div class="carousel-item">
      <div class="row d-flex align-items-end">
        <div class="col-12 col-md-4 offset-md-2">
        <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/suf_01.gif" alt="Optimizers moving on $f_1 = x^3 +y^3 +x^2 + y^2 + y$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Optimizers moving on $f_1 = x^3 +y^3 +x^2 + y^2 + y$.</strong>

    FTRL converges almost immediately to the local minima. NAG, Lion, and Adam escape local minima and decrease indefinitely, with NAG being the fastest due to velocity accumulation.
  </figcaption>


  </figure>
</div>

        </div>
        <div class="col-12 col-md-4">
        <div style="max-width:540px" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/loss_01.gif" alt="Loss over time for the cubic surface $f_1(x, y)$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Loss over time for the cubic surface $f_1(x, y)$.</strong>

    Like FTRL, SGD, NAdam and RMS fail to escape the local minima, with SGD being the slowest.
  </figcaption>


  </figure>
</div>

        </div>
      </div>
    </div>
    <div class="carousel-item">
      <div class="row d-flex align-items-end">
        <div class="col-12 col-md-4 offset-md-2">
        <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/suf_02.gif" alt="Optimizers moving on $f_2 = x^2-y^2$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Optimizers moving on $f_2 = x^2-y^2$.</strong>

    Most methods experiment free fall. By considering a similar step size for both variables, RMS is the first to produce a considerable loss reduction by decreasing $x_2$, which happens at a steady change rate. NAdam also seems to navigate towards decreasing $x_2$.
  </figcaption>


  </figure>
</div>

        </div>
        <div class="col-12 col-md-4">
        <div style="max-width:540px" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/loss_02.gif" alt="Loss over time for the quadric surface $f_2(x, y)$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Loss over time for the quadric surface $f_2(x, y)$.</strong>

    NAG starts biased towards reducing $x_1$, which looses time, but quickly accumulates velocity towards reducing $x_2$ (followed by SGD). NAdam and Adam navigate the surface at a similar pace. Finally, FTRL presents the worst solution: quickly optimizing $x_1$, and slowly decreasing $x_2$.
  </figcaption>


  </figure>
</div>

        </div>
      </div>
    </div>
    <div class="carousel-item">
      <div class="row d-flex align-items-end">
        <div class="col-12 col-md-4 offset-md-2">
        <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/suf_03.gif" alt="Optimizers moving on $f_3 = x^2-y^2 -xy$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Optimizers moving on $f_3 = x^2-y^2 -xy$.</strong>

    This one behaves quite similarly to $f_2$, but the initial solution alleviates NAG's bias towards $x_1$, increasing its initial velocity.
  </figcaption>


  </figure>
</div>

        </div>
        <div class="col-12 col-md-4">
        <div style="max-width:540px" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/loss_03.gif" alt="Loss over time for the quadric surface $f_3(x, y)$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Loss over time for the quadric surface $f_3(x, y)$.</strong>

    
  </figcaption>


  </figure>
</div>

        </div>
      </div>
    </div>
    <div class="carousel-item">
      <div class="row d-flex align-items-end">
        <div class="col-12 col-md-4 offset-md-2">
        <div style="" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/suf_04.gif" alt="Optimizers moving on $f_4=\sin(\frac{x}{5})-\cos(\frac{y}{10})$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Optimizers moving on $f_4=\sin(\frac{x}{5})-\cos(\frac{y}{10})$.</strong>

    SGD is the slowest, while RMS is the fastest solution for this search space, followed by NAG, NAdam, and Adam. Lamb varies indefinitely due to its high $\eta$.
  </figcaption>


  </figure>
</div>

        </div>
        <div class="col-12 col-md-4">
        <div style="max-width:540px" class="text-center">
  <figure class="figure ">
    <img src="/assets/images/posts/ml/optimizers/loss_04.gif" alt="Loss over time for the ramp surface $f_4(x, y)$." class="figure-img img-fluid rounded mx-auto d-block w-100" />

    
  <figcaption class="figure-caption text-start">
    <strong class="title">Loss over time for the ramp surface $f_4(x, y)$.</strong>

    FTRL is the most affected to noise (and local minima points), but also converges to the global minimum, eventually.
  </figcaption>


  </figure>
</div>

        </div>
      </div>
    </div>
  </div>
  <button class="carousel-control-prev" type="button" data-bs-target="#carouselSurfaceWOpts" data-bs-slide="prev">
    <span class="carousel-control-prev-icon" aria-hidden="true"></span>
    <span class="visually-hidden">Previous</span>
  </button>
  <button class="carousel-control-next" type="button" data-bs-target="#carouselSurfaceWOpts" data-bs-slide="next">
    <span class="carousel-control-next-icon" aria-hidden="true"></span>
    <span class="visually-hidden">Next</span>
  </button>
</div>

<h2 id="final-considerations">Final Considerations</h2>
<p>In this post, I tried to bring a more intuitive visual representation for the optimization process conducted
by the various optimization methods found in modern Machine Learning literature.</p>

<p>While we focused on 3-dimensional toy problems, techniques for visualization of the optimization landscape
of higher order problems have become increasingly more popular nowadays.
You can read more on this topic on the paper “<a href="https://arxiv.org/pdf/1712.09913">Visualizing the Loss Landscape of Neural Nets</a>” <a class="citation" href="#li2018visualizing">[2]</a>,
which describes early strategies for representing high-order optimization landscapes.
Today, visualization tools have evolved to even more complex strategies, many of which can be found at the <a href="https://losslandscape.com/">losslandscape</a> website.</p>

<p>Finally, while AdamW is the go-to optimizer used in literature (although SGD with momentum is often employed in Kaggle competitions),
it is interesting to understand the motivation and objective behind each method.
By detailing them and showing their application with visual examples, it is my hope that the reader can better
understand their inner workings, as well as their assumptions, advantages, and shortcomings.</p>

<h2 id="references">References</h2>

<ol class="bibliography"><li><span id="agarap2018deep">A. F. Agarap, “Deep learning using rectified linear units (relu),” <i>arXiv preprint arXiv:1803.08375</i>, 2018.</span></li>
<li><span id="li2018visualizing">H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, “Visualizing the loss landscape of neural nets,” <i>Advances in neural information processing systems</i>, vol. 31, 2018,Available at: https://arxiv.org/pdf/1712.09913</span></li>
<li><span id="bottou2012stochastic">L. Bottou, “Stochastic gradient descent tricks,” in <i>Neural Networks: Tricks of the Trade: Second Edition</i>, Springer, 2012, pp. 421–436.</span></li>
<li><span id="bro2014principal">R. Bro and A. K. Smilde, “Principal Component Analysis,” <i>Analytical methods</i>, vol. 6, no. 9, pp. 2812–2831, 2014.</span></li>
<li><span id="lee1998independent">T.-W. Lee and T.-W. Lee, <i>Independent component analysis</i>. Springer, 1998.</span></li>
<li><span id="shlens2014tutorial">J. Shlens, “A tutorial on independent component analysis,” <i>arXiv preprint arXiv:1404.2986</i>, 2014.</span></li>
<li><span id="thomas2021momentum">G. G. Thomas Lee and E. Henning, “Momentum,” <i>SYSEN5800 Fall 2021</i>. 2021.Available at: https://optimization.cbe.cornell.edu/index.php?title=Momentum</span></li>
<li><span id="goh2017whymomentumworks">G. Goh, “Why Momentum Really Works,” <i>Distill</i>, 2017, doi: 10.23915/distill.00006.</span></li>
<li><span id="nesterov1983method">Y. Nesterov, “A method for solving the convex programming problem with convergence rate O (1/k2),” in <i>Dokl akad nauk Sssr</i>, 1983, vol. 269, p. 543.</span></li>
<li><span id="dozat2016nadam">T. Dozat, “Incorporating Nesterov Momentum into Adam,” 2016,Available at: https://cs229.stanford.edu/proj2015/054_report.pdf</span></li>
<li><span id="hinton2012rmsprop">G. Hinton, N. Srivastava, and K. Swersky, “Neural Networks for Machine Learning: Lecture 6a - Overview of mini-‐batch gradient descent.” Lecture notes.Available at: http://www.cs.toronto.edu/t̃ijmen/csc321/slides/lecture_slides_lec6.pdf</span></li>
<li><span id="duchi2011adaptive">J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online learning and stochastic optimization.,” <i>Journal of machine learning research</i>, vol. 12, no. 7, 2011.</span></li>
<li><span id="daniel2021adagrad">D. Villarraga, “AdaGrad,” <i>Cornell University Computational Optimization Open Textbook</i>. 2021.Available at: https://optimization.cbe.cornell.edu/index.php?title=AdaGrad</span></li>
<li><span id="kingma2014adam">D. P. Kingma, “Adam: A method for stochastic optimization,” <i>arXiv preprint arXiv:1412.6980</i>, 2014.</span></li>
<li><span id="goodfellow2016deeplearning">I. Goodfellow, Y. Bengio, and A. Courville, <i>Deep Learning</i>. MIT Press, 2016.</span></li>
<li><span id="dozat2016incorporating">T. Dozat, “Incorporating nesterov momentum into adam,” 2016.</span></li>
<li><span id="chen2024symbolic">X. Chen <i>et al.</i>, “Symbolic discovery of optimization algorithms,” <i>Advances in neural information processing systems</i>, vol. 36, 2024.</span></li>
<li><span id="krizhevsky2014one">A. Krizhevsky, “One weird trick for parallelizing convolutional neural networks,” <i>arXiv preprint arXiv:1404.5997</i>, 2014.</span></li>
<li><span id="you2017scaling">Y. You, I. Gitman, and B. Ginsburg, “Scaling SGD Batch Size to 32K for ImageNet Training,” <i>arXiv preprint arXiv:1708.03888</i>, vol. 6, no. 12, p. 6, 2017.</span></li>
<li><span id="you2019large">Y. You <i>et al.</i>, “Large batch optimization for deep learning: Training bert in 76 minutes,” <i>arXiv preprint arXiv:1904.00962</i>, 2019.</span></li>
<li><span id="garrigos2023handbook">G. Garrigos and R. M. Gower, “Handbook of convergence theorems for (stochastic) gradient methods,” <i>arXiv preprint arXiv:2301.11235</i>, 2023.</span></li>
<li><span id="ghadimi2013stochastic">S. Ghadimi and G. Lan, “Stochastic first-and zeroth-order methods for nonconvex stochastic programming,” <i>SIAM journal on optimization</i>, vol. 23, no. 4, pp. 2341–2368, 2013.</span></li>
<li><span id="mcmahan2013ad">H. B. McMahan <i>et al.</i>, “Ad click prediction: a view from the trenches,” in <i>Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</i>, 2013, pp. 1222–1230.</span></li></ol>

        </div>
      </article>

      
      <div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://lucasdavid-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

      
    </div>
  </div>
</div>
<div class="empty-v-space d-none d-xl-block" style="margin-bottom: 10vh"></div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css"
  integrity="sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js"
  integrity="sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js"
  integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous"
  onload="renderMathInElement(document.body, {delimiters: [{left: '$$', right: '$$', display: true},{left: '\\[', right: '\\]', display: true}, {left: '$', right: '$', display: false},{left: '\\(', right: '\\)', display: false}]});"></script>

<script src="https://cdn.jsdelivr.net/npm/anchor-js/anchor.min.js"></script>
<script>
  anchors.options = { icon: '#' };
  anchors.add();
</script>


  <!-- <svg id="visual" viewBox="0 0 1980 300"  class="curve-container__curve curve-three" xmlns="http://www.w3.org/2000/svg"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1">
  <path
    d="M0 69L55 90.2C110 111.3 220 153.7 330 169.8C440 186 550 176 660 165.5C770 155 880 144 990 139.2C1100 134.3 1210 135.7 1320 137.7C1430 139.7 1540 142.3 1650 155.5C1760 168.7 1870 192.3 1925 204.2L1980 216L1980 301L1925 301C1870 301 1760 301 1650 301C1540 301 1430 301 1320 301C1210 301 1100 301 990 301C880 301 770 301 660 301C550 301 440 301 330 301C220 301 110 301 55 301L0 301Z"
    fill="#d3d3d3"></path>
  <path
    d="M0 89L55 107.8C110 126.7 220 164.3 330 191.3C440 218.3 550 234.7 660 243.8C770 253 880 255 990 252.3C1100 249.7 1210 242.3 1320 228.8C1430 215.3 1540 195.7 1650 174.5C1760 153.3 1870 130.7 1925 119.3L1980 108L1980 301L1925 301C1870 301 1760 301 1650 301C1540 301 1430 301 1320 301C1210 301 1100 301 990 301C880 301 770 301 660 301C550 301 440 301 330 301C220 301 110 301 55 301L0 301Z"
    fill="#6a6a6a"></path>
  <path
    d="M0 229L55 232.3C110 235.7 220 242.3 330 244.3C440 246.3 550 243.7 660 247.3C770 251 880 261 990 254.8C1100 248.7 1210 226.3 1320 223.5C1430 220.7 1540 237.3 1650 233.8C1760 230.3 1870 206.7 1925 194.8L1980 183L1980 301L1925 301C1870 301 1760 301 1650 301C1540 301 1430 301 1320 301C1210 301 1100 301 990 301C880 301 770 301 660 301C550 301 440 301 330 301C220 301 110 301 55 301L0 301Z"
    fill="#121212"></path>
</svg> -->
<footer class="page-footer text-bg-dark bg-black-subtle d-print-none">
  <div class="container">
    <div class="mt-4 mb-5">
      
        <div class="row g-1 mb-4">
          
          <div class="col">
            <div class="card bg-black-subtle border-0 border-start border-dark rounded-0">
              <div class="card-body font-small pt-0 pb-0">
                <h6 class="card-title fw-bold text-light">Keras Explainable</h6>
                <p class="card-text text-light">
                  Clean implementations for AI explaining methods in Keras.
<a href="https://github.com/lucasdavid/keras-explainable" target="_new">Code</a> and
<a href="https://lucasdavid.github.io/keras-explainable" target="_new">docs</a> are available.
                </p>
              </div>
            </div>
          </div>
          
          <div class="col">
            <div class="card bg-black-subtle border-0 border-start border-dark rounded-0">
              <div class="card-body font-small pt-0 pb-0">
                <h6 class="card-title fw-bold text-light">Supporting Study Material</h6>
                <p class="card-text text-light">
                  If you are an undergrad student and are looking for additional study material,
check out our collaborative project <a href="http://comp-ufscar.github.io/">comp-ufscar.github.io</a>.
                </p>
              </div>
            </div>
          </div>
          
          <div class="col">
            <div class="card bg-black-subtle border-0 border-start border-dark rounded-0">
              <div class="card-body font-small pt-0 pb-0">
                <h6 class="card-title fw-bold text-light">Algorithms in TensorFlow</h6>
                <p class="card-text text-light">
                  I'm implementing all algorithms I find interesting using TensorFlow.
You can check it out at <a href="https://github.com/lucasdavid/algorithms-in-tensorflow/">github.com/lucasdavid/algorithms-in-tensorflow</a>.
                </p>
              </div>
            </div>
          </div>
          
          <div class="col">
            <div class="card bg-black-subtle border-0 border-start border-dark rounded-0">
              <div class="card-body font-small pt-0 pb-0">
                <h6 class="card-title fw-bold text-light">TF-Experiment</h6>
                <p class="card-text text-light">
                  And environment to run Machine Learning experiments based on components and mixins. Available at <a href="https://github.com/lucasdavid/tf-experiment">github.com/lucasdavid/tf-experiment</a>.
                </p>
              </div>
            </div>
          </div>
          
          <div class="col">
            <div class="card bg-black-subtle border-0 border-start border-dark rounded-0">
              <div class="card-body font-small pt-0 pb-0">
                <h6 class="card-title fw-bold text-light">Mineração de Dados Complexos</h6>
                <p class="card-text text-light">
                  Information around the extension program "Mineração de Dados Complexos (in Portuguese)" is available at
<a href="https://www.ic.unicamp.br/~mdc/" target="_blank">ic.unicamp.br/~mdc/</a>.
                </p>
              </div>
            </div>
          </div>
          
        </div>
        
    </div>

    <div class="text-end mt-4">
      


<div class="fs-2">
  
    <a href="https://github.com/lucasdavid"
      target="_blank"
      ><i
      aria-label="GitHub"
      class="bi bi-github link-light"></i></a>
  
  
    <a href="https://www.linkedin.com/in/ld7"
      target="_blank"
      title="LinkedIn"><i class="bi bi-linkedin text-primary sr-none"></i></a>
  
  <span itemscope itemtype="https://schema.org/Person">
    <a itemprop="sameAs" content="https://orcid.org/0000-0002-8793-7300" href="https://orcid.org/0000-0002-8793-7300"
       target="orcid.widget"
       rel="me noopener noreferrer"
       title="ORCID"
      ><img src="/assets/images/infra/32px-ORCID_iD.webp" alt="ORCID logo" class="sr-none" style="margin-bottom: 7px; width: 32px;"></a>
  </span>
  
  <a href="http://stackoverflow.com/users/2429640/lucasdavid"
     target="_blank"
     title="Stackoverflow"><i class="bi bi-code-slash link-light sr-none"></i></a>
  
    <a href="https://youtube.com/channel/UC7IWeKUy4OSlC5ripcQGD6Q"
      target="_blank"
      title="Youtube"><i class="bi bi-youtube text-danger sr-none"></i></a>
  
    <a href="mailto:mb37410l3@mozmail.com"
      target="_blank"
      title="Mail"><i class="bi bi-envelope-fill link-light sr-none"></i></a>
  
  <a href="assets/docs/lucas-david-resume.pdf"
     target="_blank"
     title="Resume"><i class="bi bi-person-lines-fill link-light sr-none"></i></a>
</div>

    </div>
    <div class="text-end">
      <p>
        ® Lucas David. Todos os direitos reservados.
      </p>
    </div>
</div>
</footer>

  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/umd/popper.min.js"
    integrity="sha384-I7E8VVD/ismYTF4hNIPjVp/Zjvgyol6VFvRkX/vR+Vc4jQkC+hVqc2pM8ODewa9r"
    crossorigin="anonymous" defer></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js"
    integrity="sha384-0pUGZvbkm6XF6gxjEnlmuGrJXVbNuzT9qBBavbLwCsOGabYfZo0T0to5eqruptLy"
    crossorigin="anonymous" defer></script>

</body>
</html>
